{"componentChunkName":"component---src-templates-post-js","path":"/blog/bert-nlp","result":{"data":{"markdownRemark":{"html":"<p>BERT (Bidirectional Encoder Representations from Transformers) is a recent <a href=\"https://arxiv.org/abs/1810.04805\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">paper</a> published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering <code class=\"language-text\">(SQuAD v1.1)</code>, Natural Language Inference (MNLI), and others.</p>\n<p>BERT's key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The paper's results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible.</p>\n<h1>Background</h1>\n<p>In the field of computer vision, researchers have repeatedly shown the value of transfer learning – pre-training a neural network model on a known task, for instance ImageNet, and then performing fine-tuning – using the trained neural network as the basis of a new purpose-specific model. In recent years, researchers have been showing that a similar technique can be useful in many natural language tasks.</p>\n<p>A different approach, which is also popular in NLP tasks and exemplified in the recent <a href=\"https://arxiv.org/abs/1802.05365\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><code class=\"language-text\">ELMo</code></a> paper, is feature-based training. In this approach, a pre-trained neural network produces word embeddings which are then used as features in NLP models.</p>\n<h1>How BERT works</h1>\n<p>BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms – an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT's goal is to generate a language model, only the encoder mechanism is necessary. The detailed workings of Transformer are described in a <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">paper</a> by Google.</p>\n<p>As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it's non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).</p>\n<p>The chart below is a high-level description of the Transformer encoder. The input is a sequence of tokens, which are first embedded into vectors and then processed in the neural network. The output is a sequence of vectors of size <code class=\"language-text\">H</code>, in which each vector corresponds to an input token with the same index.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ce7e87c0160b265aa5e0823088b57d38/638e9/transformer.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.57142857142858%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'193\\'%20viewBox=\\'0%200%20400%20193\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M47%205l-1%208c0%2010-2%209%2028%209%2029%200%2027%201%2027-9%200-11%202-10-27-10-25%200-26%200-27%202m74%200c-3%204-3%2013%200%2016%202%202%203%202%2014%202h12v9c0%2010%201%2012%201%202l1-7v-2c-1-2%200-2%209-2%2015%200%2015%200%2015-10%200-7%200-8-2-9l-25-1c-24%200-24%200-25%202m76-1c-2%201-2%203-2%209%200%2010%200%2010%2015%2010h12v9l1%209c1%201%201-1%201-9v-9h11c13%200%2014-1%2014-10%200-10%201-10-27-10l-25%201m73%200c-2%201-2%203-2%209%200%2010%200%2010%2015%2010h12v8c0%2011%202%2013%202%203v-9c0-2%201-2%2010-2%2014%200%2015-1%2015-10%200-10%201-10-27-10l-25%201m72%200v17c2%202%2050%202%2052%200l1-8-1-9c-2-2-50-2-52%200M49%205c-2%200-1%2015%201%2015h47c3%200%203-15%200-15H49m294%200v15c2%202%2048%202%2050%200l1-7-1-8c-2-2-48-2-50%200M121%206v8l1%207h50V6l-25-1-26%201m75%200v8l1%207h50V6l-25-1-26%201m73%200v8l1%207h50V6l-25-1-26%201M53%2043l-2%202h2c1-1%2029-2%20169-2l168-1H222L53%2043m340%2039l1%2033a410%20410%200%2000-1-33m-135-3c0%201-4%202-5%201h-6c-1-1-2-1-3%201h-2l-5-1c-5%200-7%202-4%205%201%201%201%201%203-1h1c0%202%202%201%202-1l1-2%201%202c0%202%200%202%201%201h1l3%201%202-1h2c1%202%202%202%203%201h12c0-2%201-2%201-1l1%201%201-2%201-2v-1h-5l-3%201c-1%201-1%201-1-1l-1-2-1%201m133%2040c-2%201-13%202-169%202l-167%201h167a1279%201279%200%2000169-3m-97%205v18h-13l-12%201-1%207c0%2011%200%2011%2014%2011h12v8c0%2010%201%2012%201%203l1-7v-3l11-1c14%200%2015%200%2015-9%200-10%200-10-15-10h-12v-7l1-8v-3h-2m-222%201v17H60c-14%200-14%200-14%2010%200%209%201%209%2014%209%2010%200%2012%200%2012%202%200%2017%202%2022%202%206v-8h12c14%200%2015%200%2015-9%200-10%200-10-14-10H74v-12c1-3%200-6-1-6l-1%201m75%201v2l1%207v7h-13l-12%201-1%207c0%2011%200%2011%2014%2011h12v9c0%2010%201%2012%201%202l1-7v-2c-1-2-1-2%2011-2l13-1%201-9v-8l-13-1h-13v-7l1-7v-2c-2-2-2-2-3%200m74-1v17h-13l-12%201-1%207c0%2011%200%2011%2014%2011h12v9c0%2010%201%2012%201%202l1-7v-2c-1-2%200-2%2011-2%2014%200%2015%200%2015-9%200-10%200-10-14-10h-13v-7l1-7v-3h-2m146%200v17h-12c-14%200-14%200-14%2010%200%209%200%209%2014%209%2010%200%2012%200%2012%202%200%2017%202%2022%202%206v-8h12c14%200%2014%200%2014-9%200-10%201-10-14-10h-12v-14c1-1%200-4-1-4l-1%201M2%20131l-1%203c0%203%207%203%207%200h1l1%202%201-2h1c0%202%201%202%203%202h15c6%200%207%200%207-2h1c0%202%201%202%202%202l1%201c0%202%203%201%204-2%200-3-2-4-4-3h-2l-2-1h-7l-2%201c0-2-1-2-3-1-1%202-2%202-3%201h-2c-1%201-2%200-4-1s-2-1-2%201h-1c-2-1-3-1-6%201l-2-1c1-2-2-3-3-1m47%2013l-1%208v7h51v-15l-25-1-25%201m74%200v8l1%207h50v-15l-25-1-26%201m73%200v8l1%207h50v-15l-25-1-26%201m74%200l-1%208%201%207h50v-15l-25-1-25%201m73%200l-1%208%201%207h51v-7c0-9%201-9-26-9l-25%201\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transformer\"\n        title=\"transformer\"\n        src=\"/static/ce7e87c0160b265aa5e0823088b57d38/39600/transformer.png\"\n        srcset=\"/static/ce7e87c0160b265aa5e0823088b57d38/1aaec/transformer.png 175w,\n/static/ce7e87c0160b265aa5e0823088b57d38/98287/transformer.png 350w,\n/static/ce7e87c0160b265aa5e0823088b57d38/39600/transformer.png 700w,\n/static/ce7e87c0160b265aa5e0823088b57d38/638e9/transformer.png 1040w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>When training language models, there is a challenge of defining a prediction goal. Many models predict the next word in a sequence (e.g. \"The child came home from ___\"), a directional approach which inherently limits context learning. To overcome this challenge, BERT uses two training strategies:</p>\n<h2>Masked Language Model (MLM)</h2>\n<p>Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a <code class=\"language-text\">[MASK]</code> token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:</p>\n<ol>\n<li>\n<h4><span style=\"font-weight: normal\"> Adding a classification layer on top of the encoder output.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Calculating the probability of each word in the vocabulary with softmax.</h4>\n</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/940d4105b208b0abf1cdef8b39ff316d/e91c1/mlm.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'271\\'%20viewBox=\\'0%200%20400%20271\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M50%207c-2%203-1%2015%201%2016h50c2-1%203-15%201-17L76%205C52%205%2051%205%2050%207m75-1l-1%2010%201%208h25c28%200%2027%200%2027-9%200-10%201-10-27-10l-25%201m72%201c-2%203-3%2011-1%2014l1%203h51v-8c0-12%202-11-26-11-24%200-24%200-25%202m72%200c-2%201-2%203-2%207%200%2010%200%2010%2015%2010%2012%200%2013%200%2012%202v16h-70v-8c0-8%200-10-2-7v2l1%207v6h-71v-8c1-8%200-11-2-7v2l1%207v6H78l-1-4%201-7c1-3%200-6-1-6l-1%2010v7H65a16844%2016844%200%2000314%200h-11v-8c0-8%200-10-2-9v17h-70V24h24v-8l-1-10c-2-2-48-2-50%201m71-1c-2%202-1%2014%201%2016%201%202%205%202%2025%202%2028%200%2027%200%2027-9%200-11%202-10-27-10l-26%201M52%207l-1%208v7h50V7L76%206%2052%207m290%200l-1%208v7h50V7l-25-1-24%201M125%208v8l1%207h49V8l-25-1-25%201m72%200v8l1%207h49V8l-25-1-25%201M7%2021c-1%200-2%201-2%203%200%203%200%203%204%203%202%200%203%200%203-2l1%201%201%202v-2c0-2%200-2%201-1%201%202%2011%202%2011%201h6c2%202%204%201%204-2s-2-3-4-1h-1c0-2-4-2-4%200h-1l-2-1h-7l-7-1H7m235%2031l-2%201h-8c-1%202-2%202-2%201-1-2-9-2-10%200%200%201-1%201-1-1-2-2-5%200-5%202%200%203%203%204%204%202h2c1%201%201%201%202-1s1-2%201%200%204%203%204%200%202-2%202%200c0%201%200%202%201%201h7l2%201%202-1-1-1h-2l2-2%202%202c0%201%201%202%207%202h7v-4l-1-3-1%201-1%201h-4c-2-1-3%200-4%203-1%201-1%200-1-2%200-3-2-5-2-2M65%2066h11v17H64c-14%200-15%201-15%2010%200%2010-1%209%2027%209l26-1%201-8c0-10-1-10-14-10H77l1-7v-8c-1-2%201-2%2036-2l37%201-1%202v2l1%206v7h-13l-13%201-1%207c0%2011%200%2011%2015%2011%2012%200%2012%200%2011%202v2l1%207c0%2010%201%208%201-2l1-9h10c13%200%2013%200%2014-8%200-11%200-11-14-11h-11V66h36l35%201-1%202v2l1%207v6h-26l-1%203c-2%203-1%2011%201%2014%201%202%202%202%2014%202s12%200%2011%202v2l1%207c0%2010%201%208%201-2v-9h11c13%200%2013%200%2013-11v-7l-12-1h-12V66h36c27%200%2035%200%2034%201v17h-12c-15%200-15%200-15%2010%200%209%201%209%2016%209%209%200%2012%200%2011%201-1%202%200%2015%201%2016s2-10%201-15c0-2%201-2%2011-2%2013%200%2013%200%2013-11v-7l-12-1h-12V66h70v17h-12c-15%200-15%200-15%2010l1%208c2%202%2050%202%2052%200l1-8c0-10-1-10-14-10h-11V66h11a17200%2017200%200%2000-314%200M51%2086v8l1%207h49V85H77l-26%201m74%200v8l1%207h49V86l-25-1-25%201m72%200v8l1%207h49V86l-25-1-25%201m73%200l-1%208v7h50v-7l-1-8h-48m71%207l1%208h49V85h-50v8m-83%2065l-5%202-2-1-2%201h-13c-1-2-4%200-4%202%201%203%204%204%204%201l1%201c2%201%202%201%202-1l1-2%201%202c0%202%200%202%201%201%200-2%200-2%201%200h4l-1-1c-2%200-3-1-1-2%200-1%202%200%203%202%202%202%203%202%204%201h14c1%201%201%200%201-1l1-2%201-1-1-1-2%201h-1l-3-1-3-1h-1m35%2045v17h-11c-14%200-15%201-15%209%200%2010%200%2010%2014%2010h12v8l1%209c1%201%201-1%201-9v-8h25v-8c0-11%200-11-14-11h-11v-12c1-3%200-6-1-6l-1%201m-218%201v16H64c-14%200-15%201-15%2010s0%209%2014%209c12%200%2012%200%2011%202v2l1%207%201%208v-8l1-7v-2c-1-2-1-2%2011-2%2015%200%2015%200%2015-9%200-10%200-10-15-10H76v-17l-1%201m74%200v16h-11c-14%200-14%200-14%2011l1%208h24v9c0%208%200%2010%201%209l1-9v-9h12c14%200%2014%200%2014-9s-1-10-15-10h-11v-7l-1-10-1%201m72%200v16h-11c-14%200-15%201-15%2010s0%209%2014%209h12v9l1%209c1%201%201-1%201-9v-9h25v-8c0-11%200-11-14-11h-11v-7l-1-10-1%201m144%200v16h-11c-15%200-15%200-15%2010%200%209%200%209%2014%209%2011%200%2012%200%2012%202-1%204%200%2016%201%2016v-7l1-6v-3c0-2%200-2%2012-2%2014%200%2014%200%2014-9%200-10%200-10-15-10h-12v-6l1-8-1-3-1%201M5%20212c0%203%200%203%203%203s4-1%204-2c1-1%201-1%201%201v2l1-2%201-1c0%202%2010%203%2011%201h1l5%201c4%200%204%200%204-3s0-3-2-2c-3%202-3%202-3%200h-2c-2%201-3%202-4%201l-2-1h-9c-3%200-3%200-4%202l-2%202v-3c-1-1-1-1%201-1%201-1%201-1-1-1-3%200-3%200-3%203m46%2011v8l1%207h49v-16H77l-26%201m74%200v8l1%207h49v-15l-25-1-25%201m72%200v8l1%207h49v-15l-25-1-25%201m72%200v8l1%207h49v-15l-25-1-25%201m72%207l1%208h49v-16h-50v8m-36-3h-4l-1%202h-1l-2-2c-2%200-3%203-1%203v1l-2-2c-1-3-3-3-4%200l-1%203v-3c0-2-3-3-3-1h-1c-3-2-6-1-6%203l1%203c1%200%202-1%201-3%200-3%200-3%201-1%201%203%203%203%204%202h1l8%201%207-1%202%201c3%202%204%202%204-2s-1-5-3-4\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"mlm\"\n        title=\"mlm\"\n        src=\"/static/940d4105b208b0abf1cdef8b39ff316d/39600/mlm.png\"\n        srcset=\"/static/940d4105b208b0abf1cdef8b39ff316d/1aaec/mlm.png 175w,\n/static/940d4105b208b0abf1cdef8b39ff316d/98287/mlm.png 350w,\n/static/940d4105b208b0abf1cdef8b39ff316d/39600/mlm.png 700w,\n/static/940d4105b208b0abf1cdef8b39ff316d/e91c1/mlm.png 876w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness (see Takeaways <code class=\"language-text\">#3</code>).</p>\n<p>Note: In practice, the BERT implementation is slightly more elaborate and doesn't replace all of the <code class=\"language-text\">15%</code> masked words. See <a href=\"#appendix-a\"><code class=\"language-text\">Appendix A</code></a> for additional information.</p>\n<h2>Next Sentence Prediction (NSP)</h2>\n<p>In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, <code class=\"language-text\">50%</code> of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other <code class=\"language-text\">50%</code> a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.</p>\n<p>To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:</p>\n<ol>\n<li>\n<h4><span style=\"font-weight: normal\"> A <code class=\"language-text\">[CLS]</code> token is inserted at the beginning of the first sentence and a <code class=\"language-text\">[SEP]</code> token is inserted at the end of each sentence.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of <code class=\"language-text\">2</code>.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.</h4>\n</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/31be80bc014c1d9aef9bfa68474b0f7f/8d9c7/nsp.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.428571428571427%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'126\\'%20viewBox=\\'0%200%20400%20126\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M126%206l-1%203h20l-1-3c0-4%200-4-4-4l-4%201h-7c0-3-3-1-3%203m147-1c0%202-1%203-2%203-1%201%204%201%2010%201%2014%200%2014%200%2014%209v7h-26v-8l-1-7-1%209v7h30v-7c0-8-1-11-5-11l-1-3c0-2%200-3-2-3v1l1%201h-2c-1-2-2-2-2-1h-1l-1-1-1%202c1%202%200%201-2-1h-1l-1%201V3l-1-1-1%201h-1c0-3-3-1-3%202M152%209l-1%209v8h25v-8l-1-9h-23m178%201c-2%201-2%204-2%209v7h35v-7c0-11%200-11-17-11-13%200-15%200-16%202m36-1l-1%209v8h29v-8l-1-9h-27m-213%201l-1%208v7h23v-7c0-9%200-9-11-9l-11%201m178%200l-1%208v7h31v-7l-1-8h-29m36%200l-1%208v7h27v-7l-1-8h-25M86%2031L2%2032a21523%2021523%200%200084-1M64%2045v9h27V36H64v9m29%200v9h26V36H93v9m29%200v9h26V36h-26v9m29%200v9h25V36h-25v9m29%200v9h25V36h-25v9m28%200v9h30V36h-30v9m32%200v9h26V36h-26v9m29%200v9h25V36h-25v9m30%200v9h25V36h-25v9m30%200v9h32V36h-32v9m36%200v9h29V36h-29v9M65%2045v8h25V37H65v8m29%200v8h24V37H94v8m29%200v8h24V37h-24v8m29%200v8h23V37h-23v8m29%200v8h23V37h-23v8m28%200v8h28V37h-28v8m32%200v8h24V37h-24v8m29%200v8h23V37h-23v8m30%200v8h23V37h-23v8m30%200v8h30V37h-30v8m36%200v8h27V37h-27v8M5%2049c0%203%204%204%205%201h3c1%202%2020%202%2021%200l2%201c1%201%202%202%203%201h3l1-1v-2c0-1-13-2-15-1h-9l-3-1h-1c0%201-1%202-3%201l-3%201-1%201v-2l-1-2c-2%200-2%201-2%203m59%2028v9h26V68H64v9m29%200v9h26V68H93v9m29%200v9h26V68h-26v9m29%200v9h25V68h-25v9m29%200v9h25V68h-25v9m28%200v9h30V68h-30v9m32%200v9h26V68h-26v9m29%200v9h25V68h-25v9m30%200v9h25V68h-25v9m30%200v9h32V68h-32v9m36%200v9h29V68h-29v9M66%2077v8h22V69H66v8m29%200v8h23V69H95v8m29%200v8h22V69h-22v8m28%200v8h23V69h-23v8m29%200v8h23V69h-23v8m29%200v8h26V69h-26v8m32%200v8h6c6%200%207%200%206-2%200-2-1-3-3-2-3%200-3%200-3-4s0-4%203-4%204%202%201%202c-2%200-2%200%200%201%202%200%202%201%201%201l-2%201h-1l4%201c4%200%204%200%204%203-1%203%200%203%203%203h3V69h-22v8m28%200v8h7c6%200%207%200%206-1v-2l-3-1c-3%200-3%200-3-4s0-4%203-4c2%200%203%202%200%202v1l2%201-1%201h-2l2%201h3c3%201%204%204%202%205-1%201%200%201%203%201h4V69h-23v8m30%200v8h7c4%200%207%200%206-1l-1-2-2-1c-3%200-3%200-3-4s0-4%203-4c2%200%203%202%200%202v1c1%200%202%200%201%201h-1l-1%201%204%201c4%201%206%203%203%205-1%201%200%201%203%201h4V69h-23v8m30%200v8h8c9%200%209%200%208-2%200-2-1-3-3-2-3%200-3%200-3-4s0-4%203-4c4%200%204%201%201%202l-2%201h2c1%200%202%200%201%201l-2%201h-1l4%201c4%200%204%200%204%203l-1%203h11V69h-30v8m36%200v8h8c5%200%208%200%207-1l-1-2-2-1c-3%200-3%200-3-4s0-4%203-4c2%200%203%202%200%202l1%201c1%201%201%201-1%201-3%201-2%202%202%202s7%203%204%205c-1%201%201%201%204%201h5V69h-27v8M5%2080c0%202%201%202%203%202l2-1h3c0%201%206%202%209%201h6l6-1%202%201c2%203%204%202%204-2l-4-1-4-1H13l-4%201-2%202v-1l1-1v-1c1-1%201-1-1-1-1%200-2%201-2%203m1%2023v3l1-3%201-2v2c0%202%200%202%201%201h1c2%202%205%201%205-1h1c0%201%202%202%207%202%204%200%206-1%206-2%201-1%201-1%201%201v2l1-2%201-2v4l1-2%201-1c0%202%207%202%207%200%201-2-8-3-18-2l-14-1c-4%200-4%200-3%203m58%209v9h26v-18H64v9m29%200v9h26v-18H93v9m29%200v9h26v-18h-26v9m29%200v9h25v-18h-25v9m29%200v9h25v-18h-25v9m28%200v9h30v-18h-30v9m32%200v9h26v-18h-26v9m29%200v9h25v-18h-25v9m30%200v9h25v-18h-25v9m30%200v9h32v-18h-32v9m36%200v9h29v-18h-29v9m-300%200v8h24v-16H65v8m29%200v8h24v-16H94v8m29%200v8h24v-16h-24v8m29%200v8h23v-16h-23v8m29%200v8h23v-16h-23v8m28%200v8h28v-16h-28v8m32%200v8h24v-16h-24v8m29%200v8h23v-16h-23v8m30%200v8h23v-16h-23v8m30%200v8h30v-16h-30v8m36%200v8h27v-16h-27v8M5%20111c0%202%200%202%202%201h21l4%201c3%200%203%200%203-3%200-1%200-2-1-1l-6%201-6-1c-1-1-1%200-1%201%200%202%200%202-1%201%200-2-3-4-3-2l-4%201-4-1-2-1c-1%200-2%201-2%203m0%208c1%203%202%204%204%201h4c1%201%207%202%209%201h5c1%201%207%200%207-1l2%201c2%203%204%202%204-2l-4-1-4-1H13l-4%201-2%202v-1l1-1v-1c1-1%201-1-1-1s-2%200-2%203\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"nsp\"\n        title=\"nsp\"\n        src=\"/static/31be80bc014c1d9aef9bfa68474b0f7f/39600/nsp.png\"\n        srcset=\"/static/31be80bc014c1d9aef9bfa68474b0f7f/1aaec/nsp.png 175w,\n/static/31be80bc014c1d9aef9bfa68474b0f7f/98287/nsp.png 350w,\n/static/31be80bc014c1d9aef9bfa68474b0f7f/39600/nsp.png 700w,\n/static/31be80bc014c1d9aef9bfa68474b0f7f/57cd1/nsp.png 1050w,\n/static/31be80bc014c1d9aef9bfa68474b0f7f/8d9c7/nsp.png 1174w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>To predict if the second sentence is indeed connected to the first, the following steps are performed:</p>\n<ol>\n<li>\n<h4><span style=\"font-weight: normal\"> The entire input sequence goes through the Transformer model.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> The output of the <code class=\"language-text\">[CLS]</code> token is transformed into a <code class=\"language-text\">2×1</code> shaped vector, using a simple classification layer (learned matrices of weights and biases).</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Calculating the probability of IsNextSequence with softmax.</h4>\n</li>\n</ol>\n<p>When training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies.</p>\n<h1>How to use BERT (Fine-tuning)</h1>\n<p>Using BERT for a specific task is relatively straightforward:</p>\n<p>BERT can be used for a wide variety of language tasks, while only adding a small layer to the core model:</p>\n<p>Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the <code class=\"language-text\">[CLS]</code> token.\nIn Question Answering tasks (e.g. <code class=\"language-text\">SQuAD v1.1</code>), the software receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&#x26;A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.\nIn Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label.\nIn the fine-tuning training, most hyper-parameters stay the same as in BERT training, and the paper gives specific guidance (Section <code class=\"language-text\">3.5</code>) on the hyper-parameters that require tuning. The BERT team has used this technique to achieve state-of-the-art results on a wide variety of challenging natural language tasks, detailed in Section <code class=\"language-text\">4</code> of the paper.</p>\n<p>Note: A pre-trained model of BERT can also be used for generating text embeddings, similarly to many other feature-based models, such as <code class=\"language-text\">doc2vec</code> and <code class=\"language-text\">ELMo</code>. The paper found that the best embeddings are achieved by concatenating the last four layers of the encoder.</p>\n<h1>Takeaways</h1>\n<p>Model size matters, even at huge scale. BERT LARGE, with <code class=\"language-text\">345 million</code> parameters, is the largest model of its kind. It is demonstrably superior on small-scale tasks to BERT BASE, which uses the same architecture with \"only\" <code class=\"language-text\">110 million</code> parameters.</p>\n<p>With enough training data, more training steps == higher accuracy. For instance, on the MNLI task, the BERT BASE accuracy improves by <code class=\"language-text\">1.0%</code> when trained on <code class=\"language-text\">1M</code> steps (<code class=\"language-text\">128,000</code> words batch size) compared to <code class=\"language-text\">500K</code> steps with the same batch size.</p>\n<p>BERT's bidirectional approach (MLM) converges slower than left-to-right approaches (because only <code class=\"language-text\">15%</code> of words are predicted in each batch) but bidirectional training still outperforms left-to-right training after a small number of pre-training steps.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6c9a7298edeb1f547d2637fbedadf105/6de9d/size.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.28571428571429%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'264\\'%20viewBox=\\'0%200%20400%20264\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M53%20112l1%20107h319v-76a362%20362%200%20012-76c-2%200-2-4-2-22%200-21%200-22%202-22l2-2h-1c-1%203-3%201-3-2V6H53v106m2%200v105h116v-49h200v-50a1139%201139%200%2000-2-49c2-2%201-3-2-3l-2-1h2c3%200%203%200%202-1v-2l1%201c1%201%201-8%201-19%200-20%200-21-2-21l-2-1h2l2-1-9-1h-8l7-1%208-1h1l1-5V7H55v105m250-46c2%202%201%202-21%203a311%20311%200%2000-14%202l35-1v2l2-2h3c2%203%204%202%202%200s-2-2%2011-2h13l-13-1c-11%200-12%200-11-2v-1l-2%202h-3c-2-3-4-2-2%200M86%20101c-1%202-3%203-3%201h-2v3c1%201-4%207-6%207l-3%202h-4c-3-3-4-2-1%200%202%202%202%203%201%204l-2%203c0%202-3%203-4%202l-2-2%201%202v4l-1%202%202-2c1-1%202-1%204%201l2%201-1-2c-2-1-2-3-1-3l2%202h1c0-2%201-2%203-2l3-1h-2l-3-1h-1l-1%201c-1-1%201-5%202-5v2l1%201h3l-2-3c-1-2-1-2%202-2l4-3%204-3%202-1c1-1%203-3%205-3%202-2%203-2%202-3-1-2-2-2-2%200l-3%201%201-2%202-3c0-2-2-1-3%202m86%2093v23h199v-47H172v24\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"size\"\n        title=\"size\"\n        src=\"/static/6c9a7298edeb1f547d2637fbedadf105/39600/size.png\"\n        srcset=\"/static/6c9a7298edeb1f547d2637fbedadf105/1aaec/size.png 175w,\n/static/6c9a7298edeb1f547d2637fbedadf105/98287/size.png 350w,\n/static/6c9a7298edeb1f547d2637fbedadf105/39600/size.png 700w,\n/static/6c9a7298edeb1f547d2637fbedadf105/57cd1/size.png 1050w,\n/static/6c9a7298edeb1f547d2637fbedadf105/4af54/size.png 1400w,\n/static/6c9a7298edeb1f547d2637fbedadf105/6de9d/size.png 1576w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h1>Compute considerations (training and applying)</h1>\n<table>\n<thead>\n<tr>\n<th align=\"left\"></th>\n<th align=\"center\">     Training Compute + Time</th>\n<th align=\"center\">     Usage Compute</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">BERT BASE</td>\n<td align=\"center\">4 Cloud TPUs, 4 days</td>\n<td align=\"center\">1 GPU</td>\n</tr>\n<tr>\n<td align=\"left\">BERT LARGE</td>\n<td align=\"center\">16 Cloud TPUs, 4 days</td>\n<td align=\"center\">1 TPU</td>\n</tr>\n</tbody>\n</table>\n<h1>Conclusion</h1>\n<p>BERT is undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. The fact that it's approachable and allows fast fine-tuning will likely allow a wide range of practical applications in the future. In this summary, we attempted to describe the main ideas of the paper while not drowning in excessive technical details. For those wishing for a deeper dive, we highly recommend reading the full article and ancillary articles referenced in it. Another useful reference is the <a href=\"https://github.com/google-research/bert\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">BERT source code</a> and models, which cover <code class=\"language-text\">103</code> languages and were generously released as open source by the research team.</p>\n<h1>Appendix A – Word Masking <span id=\"appendix-a\"><span></h1>\n<p>Training the language model in BERT is done by predicting <code class=\"language-text\">15%</code> of the tokens in the input, that were randomly picked. These tokens are pre-processed as follows – <code class=\"language-text\">80%</code> are replaced with a <code class=\"language-text\">[MASK]</code> token, <code class=\"language-text\">10%</code> with a random word, and <code class=\"language-text\">10%</code> use the original word. The intuition that led the authors to pick this approach is as follows (Thanks to Jacob Devlin from Google for the insight):</p>\n<ul>\n<li>\n<h4><span style=\"font-weight: normal\"> If we used <code class=\"language-text\">[MASK]</code> <code class=\"language-text\">100%</code> of the time, the model wouldn't necessarily produce good token representations for non-masked words. The non-masked tokens were still used for context, but the model was optimized for predicting masked words.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> If we used <code class=\"language-text\">[MASK]</code> <code class=\"language-text\">90%</code> of the time and random words <code class=\"language-text\">10%</code> of the time, this would teach the model that the observed word is never correct.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> If we used <code class=\"language-text\">[MASK]</code> <code class=\"language-text\">90%</code> of the time and kept the same word <code class=\"language-text\">10%</code> of the time, then the model could just trivially copy the non-contextual embedding.</h4>\n</li>\n</ul>\n<p>No ablation was done on the ratios of this approach, and it may have worked better with different ratios. In addition, the model performance wasn't tested with simply masking <code class=\"language-text\">100%</code> of the selected tokens.</p>","frontmatter":{"title":"BERT","description":"State of the Art Language Model for NLP","date":"2020-8-1","slug":"/blog/bert-nlp","tags":["deeplearning","nlp","transformers"]}}},"pageContext":{}},"staticQueryHashes":["3115057458"]}