{"componentChunkName":"component---src-pages-blog-index-js","path":"/blog/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"frontmatter":{"title":"TCCT Literature Survey","description":"Toxic Comment Classification using Transformers literature survey","slug":"/blog/tcct-literature-survey","date":"2020-9-1","tags":["deeplearning","nlp","transformers"],"draft":false},"html":"<h1>Research Paper Links</h1>\n<p>[1] <a href=\"https://arxiv.org/pdf/1810.04805.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/pdf/1810.04805.pdf</a></p>\n<p>[2] <a href=\"https://arxiv.org/pdf/1906.08237v2.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/pdf/1906.08237v2.pdf</a></p>\n<p>[3] <a href=\"https://arxiv.org/pdf/1910.10683.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/pdf/1910.10683.pdf</a></p>\n<p>[4] <a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894084\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894084</a></p>\n<p>[5] <a href=\"https://docplayer.net/14928867-Twitter-analytics-for-insider-trading-fraud-detection.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://docplayer.net/14928867-Twitter-analytics-for-insider-trading-fraud-detection.html</a></p>\n<p>[6] <a href=\"https://www.sciencedirect.com/science/article/pii/S0893608019302187\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.sciencedirect.com/science/article/pii/S0893608019302187</a></p>\n<p>[7] <a href=\"https://arxiv.org/pdf/1802.09957.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/pdf/1802.09957.pdf</a></p>\n<p>[8] <a href=\"https://arxiv.org/pdf/1809.07572.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/pdf/1809.07572.pdf</a></p>\n<p>[9] <a href=\"https://arxiv.org/pdf/1907.11692.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/pdf/1907.11692.pdf</a></p>\n<p>[10] <a href=\"https://www.aclweb.org/anthology/W18-4412.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.aclweb.org/anthology/W18-4412.pdf</a></p>\n<p>[11] <a href=\"https://www.aclweb.org/anthology/W17-1101.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.aclweb.org/anthology/W17-1101.pdf</a></p>\n<p>[12] <a href=\"https://csce.ucmss.com/cr/books/2018/LFS/CSREA2018/ICA4290.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://csce.ucmss.com/cr/books/2018/LFS/CSREA2018/ICA4290.pdf</a></p>\n<h1>[1]</h1>\n<p>This paper focuses on all the layers of the model, pre-trained procedures, fine-tuning the model and then perform an analysis to based on parameter basis such as GLUE score,MultiNLI accuracy and F1 score . Bert architecture is built upon top of Transformer blocks. While doing the pre processing it does the input text representation by combining the respective position ,segment and token embeddings. Due to these preprocessing steps, it makes this NLP model easily available to do finetuning to do different kinds of  NLP projects. It pre-trained the model based on two tasks i.e Masked Language Modeling and Next Sentence Prediction. All this process combines to a great pre-trained model for language understanding which is depicted by various metrics. We can even see various ablation experiment such as effect of pre-training tasks,effect of model size and feature based approach in order for better understanding their relative importance. However, this paper also suggests us that relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy.</p>\n<h1>[2]</h1>\n<p>The paper describes us about the XLNet model and also tells us how is it better than BERT. The fundamental principles behind this model involves generalised autoregressive pretraining for language understanding and the transformer XL. Where autoregressive modelling is used to predict the next word using the context words occuring either before or after the missing word. XLNet major advantage comes from permutation language modelling technique (This technique uses permutations to generate information from both the forward and backward directions simultaneously) during the pre training step. This papers also presents a fait contrast between the BERT and XLnet model. We can see best performance of three different variants of BERT and XLNet trained with the same data and hyperparameters. The analysis tells us that XLNet outperformed the BERT model. One of the reason for XLNet performing better is the use of Transformer XL which is an enhanced version of the transformer used is BERT due to the addition of components  the segment recurrence mechanism and relative encoding scheme.</p>\n<h1>[3]</h1>\n<p>In this paper, the author has discussed a very interesting concept of combining transfer learning methods for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. The framework and model is referred as \"Text-to-Text Transfer Transformer\" (T5). The paper actually highlights the importance of cleaning the data, and clearly elucidates how this was done. T5 model works on  training on unlabelled data and then fine-tuning this model on the labeled text. The baseline model is designed ensuring that encode and decoder are each similar in configuration of BERT(base) stack. While doing text pre-processing they use SentencePiece to encode text as WorPiece tokens. Inspired from the BERT model they randomly samples and then drops out 15% of tokens in the input sequence. There is a clear performance stats of this model based on different benchmark such as GLUE, SGLUE, EnRo, etc. </p>\n<h1>[4]</h1>\n<p>In 2019, Saad and Yang had aimed on producing a complete tweet sentiment analysis on the basis of ordinal regression with machine learning algorithms. The suggested model included pre-processing tweets as first step and with the feature extraction model, an effective feature was generated. The methods such as Support Vector Regression (SVR), Random Forest(RF),Multinomial logistic regression (SoftMax), a were employed for classifying the sentiment analysis. The Decision Trees were also used for task classification and regression. Moreover, twitter dataset was used for experimenting the suggested model. The performance of the model was measured by using the Mean Absolute mean and mean square error.The test results have shown that the suggested model has attained the best accuracy, and also DTs were performed well when compared over other methods.</p>\n<h1>[5]</h1>\n<p>Gann et al. selected 6,799 tokens out of 15000 tokens that occur 50 times or more in the overall dataset based on Twitter data, where each token is assigned a sentiment score, namely TSI(Total Sentiment Index), featuring itself as a positive token or a negative token.  The SVM and Decision Tree models were trained with different  training data. They used a method called the Granger Casualty and Durbin Watson Test in which the daily tweets were processed by the trained SVM and instead  of using a daily  count  of  positive  and negative  tweets  as the metric, a Sentiment  Key  Performance  Index  (SKPI)  and  stock market value time series are used as an indicator of sentiment. GCA is based on the assumption that if a variable X causes Y, then changes in X will systematically occur before changes in Y and the lagged values of X  will illustrate a statistically significant correlation with Y. DSI (Daily Sentiment Index)was also created to compute the daily positive and negative sentiment counts returned by the model. It behaves like a time derivative and spikes up and down during sentiment change. When combined with all these methods they showed best result predictions on Tweets.</p>\n<h1>[6]</h1>\n<p>In 2019, Park et al have developed a semi-supervised sentiment-discriminative objective for resolving the issue by documents partial sentiment data. The suggested model not only reflected the partial data, but also secured the local structures obtained from real data. The suggested model was evaluated on real time datasets. The results have shown that the suggested model was performing well. In 2019, Vashishtha and Susan have calculated the sentiment related to social media posts by a new set of fuzzy rules consisting of many datasets and lexicons. The developed model combined Word Sense Disambiguation and NLP models with a new unsupervised fuzzy rule-based model for categorizing the comments into negative, neutral, and positive sentiment class. The experiments were performed on 3 sentiment lexicons, four existing models, and nine freely available twitter datasets. The outcomes have shown that the introduced method was attaining the best results.</p>\n<h1>[7]</h1>\n<p>The CNN have been widely applied to image classification problems due to its capability to exploit the 'local stationarity' property of image data. It can be interpreted as the attribute of an image pixel to present dependency between neighboring pixels. The same goes for word embeddings, that is, a word in a sentence is dependent on its neighboring words of the same sentence. This dependency is exploited by training a CNN on the word embeddings and tuning it to perform classification tasks. The paper authored by Spiros et al. arrived at this conclusion that the convolutional network performs better that the well established traditional methods including SVM, KNN, NB and LDA.</p>\n<h1>[8]</h1>\n<p>This paper focuses primarily on the challenges faced when approaching the task of toxic comment classification. Some of the discussed challenges were: occurrence of out-of-vocabulary words and misspelled words, long-range dependencies and, multi-word toxic phrases. These challenges introduce significant difficulties when training models that aim to identify and classify toxic sentences. Further challenges include doubtful labels, toxicity without swear words, rhetorics and metaphors and, sarcasm and irony.</p>\n<h1>[9]</h1>\n<p>RoBERTa is a pre-training approach developed to overcome the shortcomings of BERT. The differences were: model was trained over more data, longer and with a bigger batch size; removed next sentence prediction objective; dynamically changing the masking pattern applied to training data. BERT was optimized with Adam using the following parameters: β1 = 0.9, β2 = 0.999, ǫ = 1e-6 and L2 weight decay of 0.01. The learning rate was warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. BERT was trained with a dropout of 0.1 on all layers and attention weights, and a GELU (Gaussian Error Linear Unit) activation function. Models were pre-trained for S = 1,000,000 updates, with mini-batches containing B = 256 sequences of maximum length T = 512 tokens. Results showed up to 10% jump in accuracy in GLUE, SQuAD and RACE leaderboards.</p>\n<h1>[10]</h1>\n<p>FastText, developed by the Facebook AI research (FAIR) team, is a text classification tool suitable to model text involving out of-vocabulary (OOV) words. Zhang et al showed that character level CNN works well for text classification without the need for words. It used four classification algorithms: Logistic regression, Naïve Bayes with SVM, Extreme Gradient Boosting and FastText algorithm with Bidirectional LSTM. The Bidirectional LSTM is a further improvement on the LSTM where the network can read the context in either direction and can be trained using all available input information in the past and future of a specific time. The BiLSTM model was trained on FastText skipgram embedding obtained using Facebook’s fastText algorithm.</p>\n<h1>[11]</h1>\n<p>Word embeddings and CNN are compared against BoW approach for text classification methods namely Support Vector Machines (SVM), Naive Bayes (NB), k-Nearest Neighbor (kNN) and Linear Discriminated Analysis (LDA) applied on the designed DTMs. The methods for toxic comment detection employing the dataset. There are six types of toxicity: 'toxic', 'severe toxic', 'obscene', 'threat', 'insult', 'identity hate' in the original dataset, all these categories were considered as toxic in order to convert into binary classification. Finally a statistical analysis was performed on the outcomes of the binary classification. For this purpose they considered: samples labeled as 'toxic' and predicted as 'toxic' as True Positive (TP), samples labeled as 'toxic' and predicted as 'non-toxic' as False Negative (FN), samples labeled as 'non-toxic' and predicted as 'non-toxic' as True Negative (TN) and samples labeled as 'non-toxic' and predicted as 'toxic' as False Positive (FP).</p>\n<h1>[12]</h1>\n<p>The paper questions if proceeding to build state-of-art models really worth it considering a lot of difficulties in the way. One of them include a concern about the topic being new and dedicated models are not being developed to serve the purpose. The most intimidating said challenge with the online comments data was that the words are non-standard English full of typos and spurious characters that could severely hurt the performance in classification task. Few of the models used for training the data were NBSVM, BiLSTM and XGBoost. Precision was found to be highest and recall was found to be the lowest in the XGBoost model suggesting the inadequacy of negative examples in the dataset.</p>"}},{"node":{"frontmatter":{"title":"BERT","description":"State of the Art Language Model for NLP","slug":"/blog/bert-nlp","date":"2020-8-1","tags":["deeplearning","nlp","transformers"],"draft":false},"html":"<p>BERT (Bidirectional Encoder Representations from Transformers) is a recent <a href=\"https://arxiv.org/abs/1810.04805\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">paper</a> published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering <code class=\"language-text\">(SQuAD v1.1)</code>, Natural Language Inference (MNLI), and others.</p>\n<p>BERT's key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The paper's results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible.</p>\n<h1>Background</h1>\n<p>In the field of computer vision, researchers have repeatedly shown the value of transfer learning – pre-training a neural network model on a known task, for instance ImageNet, and then performing fine-tuning – using the trained neural network as the basis of a new purpose-specific model. In recent years, researchers have been showing that a similar technique can be useful in many natural language tasks.</p>\n<p>A different approach, which is also popular in NLP tasks and exemplified in the recent <a href=\"https://arxiv.org/abs/1802.05365\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><code class=\"language-text\">ELMo</code></a> paper, is feature-based training. In this approach, a pre-trained neural network produces word embeddings which are then used as features in NLP models.</p>\n<h1>How BERT works</h1>\n<p>BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms – an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT's goal is to generate a language model, only the encoder mechanism is necessary. The detailed workings of Transformer are described in a <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">paper</a> by Google.</p>\n<p>As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it's non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).</p>\n<p>The chart below is a high-level description of the Transformer encoder. The input is a sequence of tokens, which are first embedded into vectors and then processed in the neural network. The output is a sequence of vectors of size <code class=\"language-text\">H</code>, in which each vector corresponds to an input token with the same index.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ce7e87c0160b265aa5e0823088b57d38/638e9/transformer.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.57142857142858%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'193\\'%20viewBox=\\'0%200%20400%20193\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M47%205l-1%208c0%2010-2%209%2028%209%2029%200%2027%201%2027-9%200-11%202-10-27-10-25%200-26%200-27%202m74%200c-3%204-3%2013%200%2016%202%202%203%202%2014%202h12v9c0%2010%201%2012%201%202l1-7v-2c-1-2%200-2%209-2%2015%200%2015%200%2015-10%200-7%200-8-2-9l-25-1c-24%200-24%200-25%202m76-1c-2%201-2%203-2%209%200%2010%200%2010%2015%2010h12v9l1%209c1%201%201-1%201-9v-9h11c13%200%2014-1%2014-10%200-10%201-10-27-10l-25%201m73%200c-2%201-2%203-2%209%200%2010%200%2010%2015%2010h12v8c0%2011%202%2013%202%203v-9c0-2%201-2%2010-2%2014%200%2015-1%2015-10%200-10%201-10-27-10l-25%201m72%200v17c2%202%2050%202%2052%200l1-8-1-9c-2-2-50-2-52%200M49%205c-2%200-1%2015%201%2015h47c3%200%203-15%200-15H49m294%200v15c2%202%2048%202%2050%200l1-7-1-8c-2-2-48-2-50%200M121%206v8l1%207h50V6l-25-1-26%201m75%200v8l1%207h50V6l-25-1-26%201m73%200v8l1%207h50V6l-25-1-26%201M53%2043l-2%202h2c1-1%2029-2%20169-2l168-1H222L53%2043m340%2039l1%2033a410%20410%200%2000-1-33m-135-3c0%201-4%202-5%201h-6c-1-1-2-1-3%201h-2l-5-1c-5%200-7%202-4%205%201%201%201%201%203-1h1c0%202%202%201%202-1l1-2%201%202c0%202%200%202%201%201h1l3%201%202-1h2c1%202%202%202%203%201h12c0-2%201-2%201-1l1%201%201-2%201-2v-1h-5l-3%201c-1%201-1%201-1-1l-1-2-1%201m133%2040c-2%201-13%202-169%202l-167%201h167a1279%201279%200%2000169-3m-97%205v18h-13l-12%201-1%207c0%2011%200%2011%2014%2011h12v8c0%2010%201%2012%201%203l1-7v-3l11-1c14%200%2015%200%2015-9%200-10%200-10-15-10h-12v-7l1-8v-3h-2m-222%201v17H60c-14%200-14%200-14%2010%200%209%201%209%2014%209%2010%200%2012%200%2012%202%200%2017%202%2022%202%206v-8h12c14%200%2015%200%2015-9%200-10%200-10-14-10H74v-12c1-3%200-6-1-6l-1%201m75%201v2l1%207v7h-13l-12%201-1%207c0%2011%200%2011%2014%2011h12v9c0%2010%201%2012%201%202l1-7v-2c-1-2-1-2%2011-2l13-1%201-9v-8l-13-1h-13v-7l1-7v-2c-2-2-2-2-3%200m74-1v17h-13l-12%201-1%207c0%2011%200%2011%2014%2011h12v9c0%2010%201%2012%201%202l1-7v-2c-1-2%200-2%2011-2%2014%200%2015%200%2015-9%200-10%200-10-14-10h-13v-7l1-7v-3h-2m146%200v17h-12c-14%200-14%200-14%2010%200%209%200%209%2014%209%2010%200%2012%200%2012%202%200%2017%202%2022%202%206v-8h12c14%200%2014%200%2014-9%200-10%201-10-14-10h-12v-14c1-1%200-4-1-4l-1%201M2%20131l-1%203c0%203%207%203%207%200h1l1%202%201-2h1c0%202%201%202%203%202h15c6%200%207%200%207-2h1c0%202%201%202%202%202l1%201c0%202%203%201%204-2%200-3-2-4-4-3h-2l-2-1h-7l-2%201c0-2-1-2-3-1-1%202-2%202-3%201h-2c-1%201-2%200-4-1s-2-1-2%201h-1c-2-1-3-1-6%201l-2-1c1-2-2-3-3-1m47%2013l-1%208v7h51v-15l-25-1-25%201m74%200v8l1%207h50v-15l-25-1-26%201m73%200v8l1%207h50v-15l-25-1-26%201m74%200l-1%208%201%207h50v-15l-25-1-25%201m73%200l-1%208%201%207h51v-7c0-9%201-9-26-9l-25%201\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transformer\"\n        title=\"transformer\"\n        src=\"/static/ce7e87c0160b265aa5e0823088b57d38/39600/transformer.png\"\n        srcset=\"/static/ce7e87c0160b265aa5e0823088b57d38/1aaec/transformer.png 175w,\n/static/ce7e87c0160b265aa5e0823088b57d38/98287/transformer.png 350w,\n/static/ce7e87c0160b265aa5e0823088b57d38/39600/transformer.png 700w,\n/static/ce7e87c0160b265aa5e0823088b57d38/638e9/transformer.png 1040w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>When training language models, there is a challenge of defining a prediction goal. Many models predict the next word in a sequence (e.g. \"The child came home from ___\"), a directional approach which inherently limits context learning. To overcome this challenge, BERT uses two training strategies:</p>\n<h2>Masked Language Model (MLM)</h2>\n<p>Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a <code class=\"language-text\">[MASK]</code> token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:</p>\n<ol>\n<li>\n<h4><span style=\"font-weight: normal\"> Adding a classification layer on top of the encoder output.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Calculating the probability of each word in the vocabulary with softmax.</h4>\n</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/940d4105b208b0abf1cdef8b39ff316d/e91c1/mlm.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'271\\'%20viewBox=\\'0%200%20400%20271\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M50%207c-2%203-1%2015%201%2016h50c2-1%203-15%201-17L76%205C52%205%2051%205%2050%207m75-1l-1%2010%201%208h25c28%200%2027%200%2027-9%200-10%201-10-27-10l-25%201m72%201c-2%203-3%2011-1%2014l1%203h51v-8c0-12%202-11-26-11-24%200-24%200-25%202m72%200c-2%201-2%203-2%207%200%2010%200%2010%2015%2010%2012%200%2013%200%2012%202v16h-70v-8c0-8%200-10-2-7v2l1%207v6h-71v-8c1-8%200-11-2-7v2l1%207v6H78l-1-4%201-7c1-3%200-6-1-6l-1%2010v7H65a16844%2016844%200%2000314%200h-11v-8c0-8%200-10-2-9v17h-70V24h24v-8l-1-10c-2-2-48-2-50%201m71-1c-2%202-1%2014%201%2016%201%202%205%202%2025%202%2028%200%2027%200%2027-9%200-11%202-10-27-10l-26%201M52%207l-1%208v7h50V7L76%206%2052%207m290%200l-1%208v7h50V7l-25-1-24%201M125%208v8l1%207h49V8l-25-1-25%201m72%200v8l1%207h49V8l-25-1-25%201M7%2021c-1%200-2%201-2%203%200%203%200%203%204%203%202%200%203%200%203-2l1%201%201%202v-2c0-2%200-2%201-1%201%202%2011%202%2011%201h6c2%202%204%201%204-2s-2-3-4-1h-1c0-2-4-2-4%200h-1l-2-1h-7l-7-1H7m235%2031l-2%201h-8c-1%202-2%202-2%201-1-2-9-2-10%200%200%201-1%201-1-1-2-2-5%200-5%202%200%203%203%204%204%202h2c1%201%201%201%202-1s1-2%201%200%204%203%204%200%202-2%202%200c0%201%200%202%201%201h7l2%201%202-1-1-1h-2l2-2%202%202c0%201%201%202%207%202h7v-4l-1-3-1%201-1%201h-4c-2-1-3%200-4%203-1%201-1%200-1-2%200-3-2-5-2-2M65%2066h11v17H64c-14%200-15%201-15%2010%200%2010-1%209%2027%209l26-1%201-8c0-10-1-10-14-10H77l1-7v-8c-1-2%201-2%2036-2l37%201-1%202v2l1%206v7h-13l-13%201-1%207c0%2011%200%2011%2015%2011%2012%200%2012%200%2011%202v2l1%207c0%2010%201%208%201-2l1-9h10c13%200%2013%200%2014-8%200-11%200-11-14-11h-11V66h36l35%201-1%202v2l1%207v6h-26l-1%203c-2%203-1%2011%201%2014%201%202%202%202%2014%202s12%200%2011%202v2l1%207c0%2010%201%208%201-2v-9h11c13%200%2013%200%2013-11v-7l-12-1h-12V66h36c27%200%2035%200%2034%201v17h-12c-15%200-15%200-15%2010%200%209%201%209%2016%209%209%200%2012%200%2011%201-1%202%200%2015%201%2016s2-10%201-15c0-2%201-2%2011-2%2013%200%2013%200%2013-11v-7l-12-1h-12V66h70v17h-12c-15%200-15%200-15%2010l1%208c2%202%2050%202%2052%200l1-8c0-10-1-10-14-10h-11V66h11a17200%2017200%200%2000-314%200M51%2086v8l1%207h49V85H77l-26%201m74%200v8l1%207h49V86l-25-1-25%201m72%200v8l1%207h49V86l-25-1-25%201m73%200l-1%208v7h50v-7l-1-8h-48m71%207l1%208h49V85h-50v8m-83%2065l-5%202-2-1-2%201h-13c-1-2-4%200-4%202%201%203%204%204%204%201l1%201c2%201%202%201%202-1l1-2%201%202c0%202%200%202%201%201%200-2%200-2%201%200h4l-1-1c-2%200-3-1-1-2%200-1%202%200%203%202%202%202%203%202%204%201h14c1%201%201%200%201-1l1-2%201-1-1-1-2%201h-1l-3-1-3-1h-1m35%2045v17h-11c-14%200-15%201-15%209%200%2010%200%2010%2014%2010h12v8l1%209c1%201%201-1%201-9v-8h25v-8c0-11%200-11-14-11h-11v-12c1-3%200-6-1-6l-1%201m-218%201v16H64c-14%200-15%201-15%2010s0%209%2014%209c12%200%2012%200%2011%202v2l1%207%201%208v-8l1-7v-2c-1-2-1-2%2011-2%2015%200%2015%200%2015-9%200-10%200-10-15-10H76v-17l-1%201m74%200v16h-11c-14%200-14%200-14%2011l1%208h24v9c0%208%200%2010%201%209l1-9v-9h12c14%200%2014%200%2014-9s-1-10-15-10h-11v-7l-1-10-1%201m72%200v16h-11c-14%200-15%201-15%2010s0%209%2014%209h12v9l1%209c1%201%201-1%201-9v-9h25v-8c0-11%200-11-14-11h-11v-7l-1-10-1%201m144%200v16h-11c-15%200-15%200-15%2010%200%209%200%209%2014%209%2011%200%2012%200%2012%202-1%204%200%2016%201%2016v-7l1-6v-3c0-2%200-2%2012-2%2014%200%2014%200%2014-9%200-10%200-10-15-10h-12v-6l1-8-1-3-1%201M5%20212c0%203%200%203%203%203s4-1%204-2c1-1%201-1%201%201v2l1-2%201-1c0%202%2010%203%2011%201h1l5%201c4%200%204%200%204-3s0-3-2-2c-3%202-3%202-3%200h-2c-2%201-3%202-4%201l-2-1h-9c-3%200-3%200-4%202l-2%202v-3c-1-1-1-1%201-1%201-1%201-1-1-1-3%200-3%200-3%203m46%2011v8l1%207h49v-16H77l-26%201m74%200v8l1%207h49v-15l-25-1-25%201m72%200v8l1%207h49v-15l-25-1-25%201m72%200v8l1%207h49v-15l-25-1-25%201m72%207l1%208h49v-16h-50v8m-36-3h-4l-1%202h-1l-2-2c-2%200-3%203-1%203v1l-2-2c-1-3-3-3-4%200l-1%203v-3c0-2-3-3-3-1h-1c-3-2-6-1-6%203l1%203c1%200%202-1%201-3%200-3%200-3%201-1%201%203%203%203%204%202h1l8%201%207-1%202%201c3%202%204%202%204-2s-1-5-3-4\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"mlm\"\n        title=\"mlm\"\n        src=\"/static/940d4105b208b0abf1cdef8b39ff316d/39600/mlm.png\"\n        srcset=\"/static/940d4105b208b0abf1cdef8b39ff316d/1aaec/mlm.png 175w,\n/static/940d4105b208b0abf1cdef8b39ff316d/98287/mlm.png 350w,\n/static/940d4105b208b0abf1cdef8b39ff316d/39600/mlm.png 700w,\n/static/940d4105b208b0abf1cdef8b39ff316d/e91c1/mlm.png 876w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness (see Takeaways <code class=\"language-text\">#3</code>).</p>\n<p>Note: In practice, the BERT implementation is slightly more elaborate and doesn't replace all of the <code class=\"language-text\">15%</code> masked words. See <a href=\"#appendix-a\"><code class=\"language-text\">Appendix A</code></a> for additional information.</p>\n<h2>Next Sentence Prediction (NSP)</h2>\n<p>In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, <code class=\"language-text\">50%</code> of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other <code class=\"language-text\">50%</code> a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.</p>\n<p>To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:</p>\n<ol>\n<li>\n<h4><span style=\"font-weight: normal\"> A <code class=\"language-text\">[CLS]</code> token is inserted at the beginning of the first sentence and a <code class=\"language-text\">[SEP]</code> token is inserted at the end of each sentence.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of <code class=\"language-text\">2</code>.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.</h4>\n</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/31be80bc014c1d9aef9bfa68474b0f7f/8d9c7/nsp.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.428571428571427%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'126\\'%20viewBox=\\'0%200%20400%20126\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M126%206l-1%203h20l-1-3c0-4%200-4-4-4l-4%201h-7c0-3-3-1-3%203m147-1c0%202-1%203-2%203-1%201%204%201%2010%201%2014%200%2014%200%2014%209v7h-26v-8l-1-7-1%209v7h30v-7c0-8-1-11-5-11l-1-3c0-2%200-3-2-3v1l1%201h-2c-1-2-2-2-2-1h-1l-1-1-1%202c1%202%200%201-2-1h-1l-1%201V3l-1-1-1%201h-1c0-3-3-1-3%202M152%209l-1%209v8h25v-8l-1-9h-23m178%201c-2%201-2%204-2%209v7h35v-7c0-11%200-11-17-11-13%200-15%200-16%202m36-1l-1%209v8h29v-8l-1-9h-27m-213%201l-1%208v7h23v-7c0-9%200-9-11-9l-11%201m178%200l-1%208v7h31v-7l-1-8h-29m36%200l-1%208v7h27v-7l-1-8h-25M86%2031L2%2032a21523%2021523%200%200084-1M64%2045v9h27V36H64v9m29%200v9h26V36H93v9m29%200v9h26V36h-26v9m29%200v9h25V36h-25v9m29%200v9h25V36h-25v9m28%200v9h30V36h-30v9m32%200v9h26V36h-26v9m29%200v9h25V36h-25v9m30%200v9h25V36h-25v9m30%200v9h32V36h-32v9m36%200v9h29V36h-29v9M65%2045v8h25V37H65v8m29%200v8h24V37H94v8m29%200v8h24V37h-24v8m29%200v8h23V37h-23v8m29%200v8h23V37h-23v8m28%200v8h28V37h-28v8m32%200v8h24V37h-24v8m29%200v8h23V37h-23v8m30%200v8h23V37h-23v8m30%200v8h30V37h-30v8m36%200v8h27V37h-27v8M5%2049c0%203%204%204%205%201h3c1%202%2020%202%2021%200l2%201c1%201%202%202%203%201h3l1-1v-2c0-1-13-2-15-1h-9l-3-1h-1c0%201-1%202-3%201l-3%201-1%201v-2l-1-2c-2%200-2%201-2%203m59%2028v9h26V68H64v9m29%200v9h26V68H93v9m29%200v9h26V68h-26v9m29%200v9h25V68h-25v9m29%200v9h25V68h-25v9m28%200v9h30V68h-30v9m32%200v9h26V68h-26v9m29%200v9h25V68h-25v9m30%200v9h25V68h-25v9m30%200v9h32V68h-32v9m36%200v9h29V68h-29v9M66%2077v8h22V69H66v8m29%200v8h23V69H95v8m29%200v8h22V69h-22v8m28%200v8h23V69h-23v8m29%200v8h23V69h-23v8m29%200v8h26V69h-26v8m32%200v8h6c6%200%207%200%206-2%200-2-1-3-3-2-3%200-3%200-3-4s0-4%203-4%204%202%201%202c-2%200-2%200%200%201%202%200%202%201%201%201l-2%201h-1l4%201c4%200%204%200%204%203-1%203%200%203%203%203h3V69h-22v8m28%200v8h7c6%200%207%200%206-1v-2l-3-1c-3%200-3%200-3-4s0-4%203-4c2%200%203%202%200%202v1l2%201-1%201h-2l2%201h3c3%201%204%204%202%205-1%201%200%201%203%201h4V69h-23v8m30%200v8h7c4%200%207%200%206-1l-1-2-2-1c-3%200-3%200-3-4s0-4%203-4c2%200%203%202%200%202v1c1%200%202%200%201%201h-1l-1%201%204%201c4%201%206%203%203%205-1%201%200%201%203%201h4V69h-23v8m30%200v8h8c9%200%209%200%208-2%200-2-1-3-3-2-3%200-3%200-3-4s0-4%203-4c4%200%204%201%201%202l-2%201h2c1%200%202%200%201%201l-2%201h-1l4%201c4%200%204%200%204%203l-1%203h11V69h-30v8m36%200v8h8c5%200%208%200%207-1l-1-2-2-1c-3%200-3%200-3-4s0-4%203-4c2%200%203%202%200%202l1%201c1%201%201%201-1%201-3%201-2%202%202%202s7%203%204%205c-1%201%201%201%204%201h5V69h-27v8M5%2080c0%202%201%202%203%202l2-1h3c0%201%206%202%209%201h6l6-1%202%201c2%203%204%202%204-2l-4-1-4-1H13l-4%201-2%202v-1l1-1v-1c1-1%201-1-1-1-1%200-2%201-2%203m1%2023v3l1-3%201-2v2c0%202%200%202%201%201h1c2%202%205%201%205-1h1c0%201%202%202%207%202%204%200%206-1%206-2%201-1%201-1%201%201v2l1-2%201-2v4l1-2%201-1c0%202%207%202%207%200%201-2-8-3-18-2l-14-1c-4%200-4%200-3%203m58%209v9h26v-18H64v9m29%200v9h26v-18H93v9m29%200v9h26v-18h-26v9m29%200v9h25v-18h-25v9m29%200v9h25v-18h-25v9m28%200v9h30v-18h-30v9m32%200v9h26v-18h-26v9m29%200v9h25v-18h-25v9m30%200v9h25v-18h-25v9m30%200v9h32v-18h-32v9m36%200v9h29v-18h-29v9m-300%200v8h24v-16H65v8m29%200v8h24v-16H94v8m29%200v8h24v-16h-24v8m29%200v8h23v-16h-23v8m29%200v8h23v-16h-23v8m28%200v8h28v-16h-28v8m32%200v8h24v-16h-24v8m29%200v8h23v-16h-23v8m30%200v8h23v-16h-23v8m30%200v8h30v-16h-30v8m36%200v8h27v-16h-27v8M5%20111c0%202%200%202%202%201h21l4%201c3%200%203%200%203-3%200-1%200-2-1-1l-6%201-6-1c-1-1-1%200-1%201%200%202%200%202-1%201%200-2-3-4-3-2l-4%201-4-1-2-1c-1%200-2%201-2%203m0%208c1%203%202%204%204%201h4c1%201%207%202%209%201h5c1%201%207%200%207-1l2%201c2%203%204%202%204-2l-4-1-4-1H13l-4%201-2%202v-1l1-1v-1c1-1%201-1-1-1s-2%200-2%203\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"nsp\"\n        title=\"nsp\"\n        src=\"/static/31be80bc014c1d9aef9bfa68474b0f7f/39600/nsp.png\"\n        srcset=\"/static/31be80bc014c1d9aef9bfa68474b0f7f/1aaec/nsp.png 175w,\n/static/31be80bc014c1d9aef9bfa68474b0f7f/98287/nsp.png 350w,\n/static/31be80bc014c1d9aef9bfa68474b0f7f/39600/nsp.png 700w,\n/static/31be80bc014c1d9aef9bfa68474b0f7f/57cd1/nsp.png 1050w,\n/static/31be80bc014c1d9aef9bfa68474b0f7f/8d9c7/nsp.png 1174w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>To predict if the second sentence is indeed connected to the first, the following steps are performed:</p>\n<ol>\n<li>\n<h4><span style=\"font-weight: normal\"> The entire input sequence goes through the Transformer model.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> The output of the <code class=\"language-text\">[CLS]</code> token is transformed into a <code class=\"language-text\">2×1</code> shaped vector, using a simple classification layer (learned matrices of weights and biases).</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Calculating the probability of IsNextSequence with softmax.</h4>\n</li>\n</ol>\n<p>When training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies.</p>\n<h1>How to use BERT (Fine-tuning)</h1>\n<p>Using BERT for a specific task is relatively straightforward:</p>\n<p>BERT can be used for a wide variety of language tasks, while only adding a small layer to the core model:</p>\n<p>Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the <code class=\"language-text\">[CLS]</code> token.\nIn Question Answering tasks (e.g. <code class=\"language-text\">SQuAD v1.1</code>), the software receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&#x26;A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.\nIn Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label.\nIn the fine-tuning training, most hyper-parameters stay the same as in BERT training, and the paper gives specific guidance (Section <code class=\"language-text\">3.5</code>) on the hyper-parameters that require tuning. The BERT team has used this technique to achieve state-of-the-art results on a wide variety of challenging natural language tasks, detailed in Section <code class=\"language-text\">4</code> of the paper.</p>\n<p>Note: A pre-trained model of BERT can also be used for generating text embeddings, similarly to many other feature-based models, such as <code class=\"language-text\">doc2vec</code> and <code class=\"language-text\">ELMo</code>. The paper found that the best embeddings are achieved by concatenating the last four layers of the encoder.</p>\n<h1>Takeaways</h1>\n<p>Model size matters, even at huge scale. BERT LARGE, with <code class=\"language-text\">345 million</code> parameters, is the largest model of its kind. It is demonstrably superior on small-scale tasks to BERT BASE, which uses the same architecture with \"only\" <code class=\"language-text\">110 million</code> parameters.</p>\n<p>With enough training data, more training steps == higher accuracy. For instance, on the MNLI task, the BERT BASE accuracy improves by <code class=\"language-text\">1.0%</code> when trained on <code class=\"language-text\">1M</code> steps (<code class=\"language-text\">128,000</code> words batch size) compared to <code class=\"language-text\">500K</code> steps with the same batch size.</p>\n<p>BERT's bidirectional approach (MLM) converges slower than left-to-right approaches (because only <code class=\"language-text\">15%</code> of words are predicted in each batch) but bidirectional training still outperforms left-to-right training after a small number of pre-training steps.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6c9a7298edeb1f547d2637fbedadf105/6de9d/size.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.28571428571429%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'264\\'%20viewBox=\\'0%200%20400%20264\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M53%20112l1%20107h319v-76a362%20362%200%20012-76c-2%200-2-4-2-22%200-21%200-22%202-22l2-2h-1c-1%203-3%201-3-2V6H53v106m2%200v105h116v-49h200v-50a1139%201139%200%2000-2-49c2-2%201-3-2-3l-2-1h2c3%200%203%200%202-1v-2l1%201c1%201%201-8%201-19%200-20%200-21-2-21l-2-1h2l2-1-9-1h-8l7-1%208-1h1l1-5V7H55v105m250-46c2%202%201%202-21%203a311%20311%200%2000-14%202l35-1v2l2-2h3c2%203%204%202%202%200s-2-2%2011-2h13l-13-1c-11%200-12%200-11-2v-1l-2%202h-3c-2-3-4-2-2%200M86%20101c-1%202-3%203-3%201h-2v3c1%201-4%207-6%207l-3%202h-4c-3-3-4-2-1%200%202%202%202%203%201%204l-2%203c0%202-3%203-4%202l-2-2%201%202v4l-1%202%202-2c1-1%202-1%204%201l2%201-1-2c-2-1-2-3-1-3l2%202h1c0-2%201-2%203-2l3-1h-2l-3-1h-1l-1%201c-1-1%201-5%202-5v2l1%201h3l-2-3c-1-2-1-2%202-2l4-3%204-3%202-1c1-1%203-3%205-3%202-2%203-2%202-3-1-2-2-2-2%200l-3%201%201-2%202-3c0-2-2-1-3%202m86%2093v23h199v-47H172v24\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"size\"\n        title=\"size\"\n        src=\"/static/6c9a7298edeb1f547d2637fbedadf105/39600/size.png\"\n        srcset=\"/static/6c9a7298edeb1f547d2637fbedadf105/1aaec/size.png 175w,\n/static/6c9a7298edeb1f547d2637fbedadf105/98287/size.png 350w,\n/static/6c9a7298edeb1f547d2637fbedadf105/39600/size.png 700w,\n/static/6c9a7298edeb1f547d2637fbedadf105/57cd1/size.png 1050w,\n/static/6c9a7298edeb1f547d2637fbedadf105/4af54/size.png 1400w,\n/static/6c9a7298edeb1f547d2637fbedadf105/6de9d/size.png 1576w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h1>Compute considerations (training and applying)</h1>\n<table>\n<thead>\n<tr>\n<th align=\"left\"></th>\n<th align=\"center\">     Training Compute + Time</th>\n<th align=\"center\">     Usage Compute</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">BERT BASE</td>\n<td align=\"center\">4 Cloud TPUs, 4 days</td>\n<td align=\"center\">1 GPU</td>\n</tr>\n<tr>\n<td align=\"left\">BERT LARGE</td>\n<td align=\"center\">16 Cloud TPUs, 4 days</td>\n<td align=\"center\">1 TPU</td>\n</tr>\n</tbody>\n</table>\n<h1>Conclusion</h1>\n<p>BERT is undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. The fact that it's approachable and allows fast fine-tuning will likely allow a wide range of practical applications in the future. In this summary, we attempted to describe the main ideas of the paper while not drowning in excessive technical details. For those wishing for a deeper dive, we highly recommend reading the full article and ancillary articles referenced in it. Another useful reference is the <a href=\"https://github.com/google-research/bert\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">BERT source code</a> and models, which cover <code class=\"language-text\">103</code> languages and were generously released as open source by the research team.</p>\n<h1>Appendix A – Word Masking <span id=\"appendix-a\"><span></h1>\n<p>Training the language model in BERT is done by predicting <code class=\"language-text\">15%</code> of the tokens in the input, that were randomly picked. These tokens are pre-processed as follows – <code class=\"language-text\">80%</code> are replaced with a <code class=\"language-text\">[MASK]</code> token, <code class=\"language-text\">10%</code> with a random word, and <code class=\"language-text\">10%</code> use the original word. The intuition that led the authors to pick this approach is as follows (Thanks to Jacob Devlin from Google for the insight):</p>\n<ul>\n<li>\n<h4><span style=\"font-weight: normal\"> If we used <code class=\"language-text\">[MASK]</code> <code class=\"language-text\">100%</code> of the time, the model wouldn't necessarily produce good token representations for non-masked words. The non-masked tokens were still used for context, but the model was optimized for predicting masked words.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> If we used <code class=\"language-text\">[MASK]</code> <code class=\"language-text\">90%</code> of the time and random words <code class=\"language-text\">10%</code> of the time, this would teach the model that the observed word is never correct.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> If we used <code class=\"language-text\">[MASK]</code> <code class=\"language-text\">90%</code> of the time and kept the same word <code class=\"language-text\">10%</code> of the time, then the model could just trivially copy the non-contextual embedding.</h4>\n</li>\n</ul>\n<p>No ablation was done on the ratios of this approach, and it may have worked better with different ratios. In addition, the model performance wasn't tested with simply masking <code class=\"language-text\">100%</code> of the selected tokens.</p>"}},{"node":{"frontmatter":{"title":"GSoC-2020 Proposal","description":"Proposal for Server Side Events Project","slug":"/blog/gsoc-2020-proposal","date":"2020-4-1","tags":["react","redux","node.js"],"draft":false},"html":"<h1>Project description</h1>\n<p>Traditionally, a web page has to send a request to the server to receive new data; that is, the page requests data from the server. With server-sent events, it's possible for a server to send new data to a web page at any time, by pushing messages to the web page. These incoming messages can be treated as Events + data inside the web page.</p>\n<p>This project is about building an inspector intercepting and visualizing server-sent event traffic to help the developer to easily see what exact data are received from the server and when. This inspector should be a part of the existing Network panel in Firefox DevTools. This project should ideally build upon the existing WebSocket inspection tab by adding support for server-sent events inspection.</p>\n<h1>Goals</h1>\n<ul>\n<li>\n<h4><span style=\"font-weight: normal\"> View the size of the packets and the time when it was received.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> View the payload of each event.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Support for sort and filter with respect to parameters like time and size.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> View the event headers, source and content in an organized manner as well as in JSON format.</h4>\n</li>\n</ul>\n<h1>Implementation plan</h1>\n<h2>Client side development plan:</h2>\n<ul>\n<li>\n<h4><span style=\"font-weight: normal\"> To create an interface that supports and visualizes the JSON packets that are being obtained onto the client machine. This will be added upon as an additional tab in the networks panel in the DevTools section.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> A change in the Toolbar section of the codebase to include the UI change to navigate to the tab, and a modification to the base function will be added to incorporate the same.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> An additional CSS file that contains the styles of the above mentioned tab will be appended to src/assets directory.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> A React function that governs the UI and the general framework of the panel is to be developed. Redux reducers will be used to respond to the changes by making the necessary changes to the panel.</h4>\n</li>\n</ul>\n<h2>Server side development plan:</h2>\n<ul>\n<li>\n<h4><span style=\"font-weight: normal\"> An actor is to be created that will reside on the server (browser). The code will be a part of the  devtools/server/actors/network-monitor directory on Mozilla devtools.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> This actor will be responsible for monitoring the events received and relying the information to the client via JSON objects.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> The actor will ensure the proper formatting of the objects before it is sent to the client. This includes the checks for headers, content, timestamps among others.</h4>\n</li>\n</ul>\n<h1>Architecture plan:</h1>\n<ul>\n<li>\n<h4><span style=\"font-weight: normal\"> The Remote Debugging protocol (RDP) is a Mozilla debugging protocol that allows a debugger to connect to a browser, discover what sorts of things are present to debug or inspect, select JavaScript threads to watch, and observe and modify their execution. This protocol is to be used to communicate between the client and the server.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> All communication between debugger (client) and browser (server) is in the form of JSON objects. This makes it easier to implement, debug and test the components being developed.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Client is responsible for rendering data it receives from the server and the server is responsible for collecting data and sending it over to the client with necessary information.</h4>\n</li>\n</ul>\n<h1>Sequence diagram:</h1>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4a4c49a66ee3500c94758d05a64770f5/77308/sequence.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.28571428571429%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'250\\'%20viewBox=\\'0%200%20400%20250\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M2%203v122l1%20121h394V2H200L2%203m2%201c0%202%203%203%2014%203%2011-1%2012-1%2012%201s0%202%201%200l1-2c0%202%200%202%201%201%203-2%207-1%208%202%200%202%200%202-1%201l-1-1c0%202-5%202-6%201h-1c0%202-12%201-12-1h-1c0%201-1%202-4%202s-4-1-4-2c-1-1-1-1-1%201v2l-1-2-1-1-2%202-2%202c0%202%201%202%2040%202h41l3-3%203-6V3H48L4%204m89%200l-1%203-3%206-4%203H4v228h196l196%201V3H245L93%204M36%2034l1%2012h82V22H36v12m127%200l1%2012h81V22h-82v12m109%200l1%2012h82V22h-83v12M37%2034v10h80V23H37v11m127%200v10h80V23h-80v11m109%200v10h80V23h-80v11m-79%2042l-58%201a1149%201149%200%200061%203l3-1v7c0%208%200%208%202%208%204%200%205%200%206-2a700%20700%200%2001100%200l1%2023v22h3l2%201v1l1-1%202-1%201-7c0-6%200-7%202-7s2-1%202-7v-8l3%201%203-1%204-1c2%200%203%200%203-2s3-4%205-4c2%201%203-1%203-2h-3l-3-1h-1l-1-1c0-2-1-2-8-2-9%200-9%200-9-2%200-3%200-3-5-3l-6-1h-3l-48%201h-48v-6l-1-7-11-1h-2m117%2018l1%203%201%203c0%202%200%202-1%201l-1%2017v18l3-1c2%200%203%200%203-6%200-5%200-6-2-6s-2-1-2-8c0-8%200-9%202-9%201%200%202-1%202-7%200-7-1-8-3-8-3%200-3%200-3%203m7%207c0%205%200%206%206%204h4c0%202%206%201%206-1l-1-2-2-2h-1c-1%202-5%202-5%200h1l1-1%203-1%204-1-8-1h-8v5m-124%2049l-59%201a1571%201571%200%200062%203l3-1v7c0%207%200%208%202%208l2%201h1l2-1%201-5v-6h48a479%20479%200%200151%201l2-1v26c0%2028%200%2027%206%2027h3v-10c0-8%200-9%202-9s2-1%202-7c0-8%200-9%203-7h3c0-1%201-2%204-2%202%200%203%200%203-2s1-2%203-2%202%200%201-1v-1l2%201c1%201%201%200%201-1%200-2%200-3-1-2-1%202-5%201-5-2-1-2-1-2-10-2h-8v-3c0-4-1-5-6-5l-5-1h-3l-48%201c-46%200-48%200-48-2%200-1-1-2-5-2l-7-2-2%201m117%2012l1%206v2l-1%2020v18h6v-8c0-7%200-8-2-9-2%200-2-1-2-9%200-7%200-8%202-8s3-3%201-3v-1l1-6v-7h-3c-3%200-3%200-3%205\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Sequence Diagram\"\n        title=\"Sequence Diagram\"\n        src=\"/static/4a4c49a66ee3500c94758d05a64770f5/39600/sequence.png\"\n        srcset=\"/static/4a4c49a66ee3500c94758d05a64770f5/1aaec/sequence.png 175w,\n/static/4a4c49a66ee3500c94758d05a64770f5/98287/sequence.png 350w,\n/static/4a4c49a66ee3500c94758d05a64770f5/39600/sequence.png 700w,\n/static/4a4c49a66ee3500c94758d05a64770f5/77308/sequence.png 881w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h1>Testing strategies</h1>\n<ul>\n<li>\n<h4><span style=\"font-weight: normal\"> Static code analysis using SonarQube/SonarCloud.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Unit testing for the individual functions written in Javascript.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Integration testing to ensure the rest of the components are working as expected.</h4>\n</li>\n</ul>\n<h1>Timeline</h1>\n<h4><code class=\"language-text\">May 4 - May 15</code></h4>\n<p>Familiarize myself with the code, mentor and community, the version control system, the documentation, the development environment and test system used.</p>\n<h4><code class=\"language-text\">May 16 - June 1</code></h4>\n<p>Modularize the final goal and set a deadline for each of the module completion after discussing with the mentor. Also, get the environment ready and potentially develop, test and commit a small portion to the development environment.</p>\n<h4><code class=\"language-text\">June 2 - June 15</code></h4>\n<p>Build the UI for the client side debugger to support the JSON objects received from the server. This will also include the support to sort and filter the objects received using parameters like size, timestamp and payload.</p>\n<h4><code class=\"language-text\">June 16 - August 5</code></h4>\n<p>Develop the server side actor that will essentially monitor the requests and relays the information to the debugger. Also write tests that control the quality of the code being developed.</p>\n<h4><code class=\"language-text\">August 6 - August 15</code></h4>\n<p>Polish the existing UI and improve upon it to render a neat view visualizing the details of individual events. This period is also to be used for extensive testing to make sure the components are working as expected and does not modify the behavior of other components.</p>\n<h4><code class=\"language-text\">August 16 - August 24</code></h4>\n<p>Document the written code and release the component into production after final mentor review and testing.</p>"}},{"node":{"frontmatter":{"title":"Faculty Dashboard","description":"A write-up for software engineering course group project","slug":"/blog/faculty-dashboard","date":"2020-3-1","tags":["react","node.js","mysql"],"draft":false},"html":"<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b16a2dd99a480831cca5598db5425968/21f1a/faculty.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 57.14285714285714%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'230\\'%20viewBox=\\'0%200%20400%20230\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M184%2030c-4%2011-3%2011%201%200%201-2%202-2%204%203l3%205%203-5%202-5%202%203c3%2010%206%209%202-1l-3-8-3%206-3%206-2-6-3-6-3%208m-39-5c-2%202-1%2011%201%2012%201%201%201%201-1%201-4-1-3%201%201%203s9%202%2013%200%205-5%202-3c-2%200-2%200-1-1l1-4v-2l-1%202-1%201h-1l-1-1-2-4-2-1c0%202%200%202-1%201-2-1-2-1-3%202l-2%202v1l-1%202c-2-1-1-8%201-10%201-2%200-3-2-1m0%2079h54a821%20821%200%2000-2-2c-44%200-50%200-52%202m0%2032h54a848%20848%200%2000-1-2c-45%200-51%200-53%202m2%2034c-7%204-8%2011-3%2017l4%203h49l51-1c7-3%208-11%203-16l-3-3-49-1-52%201\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Home Page\"\n        title=\"Home Page\"\n        src=\"/static/b16a2dd99a480831cca5598db5425968/39600/faculty.png\"\n        srcset=\"/static/b16a2dd99a480831cca5598db5425968/1aaec/faculty.png 175w,\n/static/b16a2dd99a480831cca5598db5425968/98287/faculty.png 350w,\n/static/b16a2dd99a480831cca5598db5425968/39600/faculty.png 700w,\n/static/b16a2dd99a480831cca5598db5425968/57cd1/faculty.png 1050w,\n/static/b16a2dd99a480831cca5598db5425968/21f1a/faculty.png 1327w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h1>Introduction</h1>\n<p>This application is built for the faculties of an institution. The web based\napplication enables faculties across different disciplines to view and edit their profile, view, modify and share their course plan and their time table. It also enables them to view the recent announcements quickly. Perhaps the most\nimportant functionality of the application is the ability to apply for leave by\ngiving their duration and reason directly through the app interface itself, thus saving a lot of hassle.\nWe hope that this application will help make the lives of people who guide\nus in our lives, a bit easier. Any queries, bug reports and feedback would be\nmuch appreciated and acted upon immediately.</p>\n<h1>Motivation</h1>\n<p>The definition for motivation is: \"a reason for doing something\".\nIn this case, it was the reason of \"gratitude\" for our faculties that motivated\nus to make something within our domain of expertise that could possibly help\nthem through some of the time-consuming tasks in their lives</p>\n<h3>Architecture: SERN stack (SQL, Express, React, Node)</h3>\n<h1>Tools Used</h1>\n<ul>\n<li>React, Node.js</li>\n<li>Git, GitHub</li>\n<li>MySQL</li>\n<li>AWS</li>\n<li>Heroku</li>\n<li>GitHub Actions</li>\n</ul>\n<h1>Implementation</h1>\n<h3>Type/Platform: Web Application.</h3>\n<h3>Stakeholder Faculty</h3>\n<p>Manage her/his profile\nUpload/view timetable\nUpload/view course plan\nView announcements\nApply Leave</p>\n<h3>Stakeholder HOD</h3>\n<p>Manage her/his profile\nUpload/view timetable\nUpload/view course plan\nMake announcements\nApply/Approve Leave</p>\n<h4>React is used for rendering the front-end.</h4>\n<h4>Node is used as the back-end to:</h4>\n<ul>\n<li>handle requests from react</li>\n<li>make required queries to the database</li>\n<li>return required response back to react</li>\n</ul>\n<h4>Amazon S3 storage is used to store data and return required data when requests are made as queries.</h4>\n<h1>Static Code Analysis</h1>\n<p>First, Sonarqube application was downloaded and a server was started in the\nlocalhost using the command “sonarqube-6.0 ./bin/linux-x86-64/sonar.sh start”,\nit also used Sonarqube scanner. Then, using our credentials we log in to the\napplication. Supported the code complexity we obtained whether the project is\npassed or failed. Once the project is passed the code analysis is seen.</p>\n<p>The tool analyses the code based on the following headers:\nDebt, Bugs, Vulnerabilities, Code smells, Coverage and duplication. We, the developers then resolved the security issues by fixing our code to satisfy the standards set by the application.</p>\n<p>Actions taken:\nAlerts in the code were considered to be a security vulnerability and this\nproblem was circumvented by adding the messages that were resolved by\nmarking the alerts as false positive.\nOther security vulnerabilities were resolved.</p>\n<h1>UI Testing</h1>\n<p>Tools Used: Selenium, Chromium Driver, Firefox Driver, Lambda Test</p>\n<p>Setup Details: First installed selenium package for python, then downloaded the Chromium driver for the current chrome browser. Lambda Test tool is used to\nperform browser and OS compatibility test.</p>\n<p>Test cases statistics: 20 test cases written</p>\n<h1>Continuous Integration</h1>\n<p>Tools used: GitHub Actions</p>\n<p>GitHub Actions has been enabled for the GitHub repository that hosts the project.\nIt has a custom webhook that triggers the GitHub Actions software which runs the preconfigured tests and deploys to GitHub Pages.</p>\n<p>We find that an automation software like GitHub Actions is extremely helpful in eliminating the routinal procedure of running tests and deployment everytime a push is initiated. This helps in saving a lot of time and effort that otherwise would be necessary for the proper functioning of the software.</p>\n<h1>Additional Software Engineering Practices</h1>\n<ul>\n<li>\n<h4><span style=\"font-weight: normal\"> Adopted material and fluidic design principles for UI.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Test suites with result portal.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Automated security updates reported through mail.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Setup a cloud enviroment to edit and push changes right through the browser without any dependencies.</h4>\n</li>\n<li>\n<h4><span style=\"font-weight: normal\"> Setup a bot that reports additional external dependencies via alerts and mails.</h4>\n</li>\n</ul>\n<h1>Conclusion</h1>\n<p>We have thus completed a faculty dashboard web application. Its features include a secure login page with Recaptcha, a page for new user registration and page to help the user in case of forgetting the password. On login, the user is redirected to his/her profile page and a navbar on the top lets the user choose to see the options: Schedule, Course plan, Leave management and Announcements and Circulars.</p>\n<p>The course plan and the faculty schedule are specific to a particular user and are stored on a cloud <code class=\"language-text\">S3</code> bucket so that it can be accessed from any device; they also include a file button to select files from local storage to update their schedule and course plan, also there are buttons to share the schedule and timetable directly over popular media and also a button to directly send the file over email.</p>\n<p>The leave management section consists of a page where all past leaves have been displayed in chronological order and a button at the bottom to apply for leave which will redirect the user to the apply leave page. Another feature included in this is: if the user is logged in as the HOD they have the ability to approve or reject the leaves, this action will be reflected on the faculty’s dashboard.</p>\n<p>There is also a page for viewing announcements and circulars. The database used to maintain all the above data is an SQL database hosted on Heroku. Finally, there is a logout button that logs the user out of the dashboard.</p>"}},{"node":{"frontmatter":{"title":"Bluetooth Case Study","description":"A case study on Bluetooth","slug":"/blog/bluetooth-case","date":"2020-2-1","tags":["bluetooth","python"],"draft":false},"html":"<h1>What is Bluetooth?</h1>\n<p>A Bluetooth technology is a high speed low powered wireless technology link that is designed to connect phones or other portable equipment together. It is a specification <code class=\"language-text\">(IEEE 802.15.1)</code> for the use of low power radio communications to link phones, computers and other network devices over short distances without wires. Wireless signals transmitted with Bluetooth cover short distances, typically up to <code class=\"language-text\">30 feet</code> (<code class=\"language-text\">10 meters</code>).</p>\n<p>It is achieved by embedded low cost transceivers into the devices. It supports on the frequency band of <code class=\"language-text\">2.45GHz</code> and can support upto <code class=\"language-text\">721KBps</code> along with three voice channels. This frequency band has been set aside by international agreement for the use of industrial, scientific and medical devices <code class=\"language-text\">(ISM).rd-compatible</code> with <code class=\"language-text\">1.0</code> devices.</p>\n<p>Bluetooth can connect up to eight devices simultaneously and each device offers a unique <code class=\"language-text\">48-bit</code> address from the <code class=\"language-text\">IEEE 802</code> standard with the connections being made point to point or multipoint.</p>\n<h1>Working of Bluetooth</h1>\n<p>Bluetooth Network consists of a Personal Area Network or a piconet which contains a minimum of <code class=\"language-text\">2</code> to maximum of <code class=\"language-text\">8</code> bluetooth peer devices - Usually a single master and upto <code class=\"language-text\">7</code> slaves.</p>\n<p>A master is the device which initiates communication with other devices. The master device governs the communications link and trafﬁc between itself and the slave devices associated with it.</p>\n<p>A slave device is the device that responds to the master device. Slave devices are required to synchronize their transmit/receive timing with that of the masters.\nIn addition, transmissions by slave devices are governed by the master device (i.e., the master device dictates when a slave device may transmit). Speciﬁcally, a slave may only begin its transmissions in a time slot immediately following the time slot in which it was addressed by the master, or in a time slot explicitly reserved for use by the slave device.</p>\n<p>The frequency hopping sequence is defined by the Bluetooth device address (<code class=\"language-text\">BD_ADDR</code>) of the master device.  The master device first sends a radio signal asking for response from the particular slave devices within the range of addresses. The slaves respond and synchronize their hop frequency as well as clock with that of the master device.</p>\n<p>Scatternets are created when a device becomes an active member of more than one piconet. Essentially, the adjoining device shares its time slots among the different piconets.</p>\n<h1>Bluetooth Addressing System</h1>\n<p>Every Bluetooth device has a unique 48-bit address, commonly abbreviated <code class=\"language-text\">BD_ADDR</code>. This will usually be presented in the form of a <code class=\"language-text\">12-digit</code> hexadecimal value. The most-significant half (<code class=\"language-text\">24 bits</code>) of the address is an organization unique identifier (OUI), which identifies the manufacturer. The lower <code class=\"language-text\">24-bits</code> are the more unique part of the address.</p>\n<h1>Bluetooth Protocol Stack</h1>\n<p>Bluetooth protocol stack defines and provides different types of layers and functionalities. Bluetooth can run the different applications over different protocol stacks, but each one of these protocol stacks uses the same Bluetooth link and physical layers. The below diagram shows a complete Bluetooth protocol stack. It shows the relationship between the protocols that use the services of other protocols when there is a payload to be transferred in the air.</p>\n<h1>Layers of Bluetooth Protocol Stack</h1>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ca442b39d7860130503187fbfe160d89/83a6d/protocol.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 71.42857142857143%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'286\\'%20viewBox=\\'0%200%20400%20286\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M25%2034l1%2010h27v17H25v19h27v54H25v19h174v17H80v10l1%2010h118v17H80v19h118v18h-81v20h166v-20h-83v-18h120v-19H200v-17h120v-20H200v-17h175v-19h-27l-1-27V80h28V61l-14-1h-14V44h28V24H25v10m1%200v8h348V25H26v9m158%200c-1%202-1%202%202%202l2%201h5l10-1c15%200%2015%200%2015-2-1-2-15-3-15%200l-2%201v-1c3-1%200-2-6-2s-7%201-7%202h-1c-1-2-3-2-3%200M54%2052v9h14l13%201v18H67l-13%201v53h145v-17h-64V97h27l1-3v-9l-1-4-13-1h-14V61h28V44H54v8m110%201v8h28v19h-28v17h72V80h-27V60h27V44h-72v9m73-1v8h28v20h-28v17h28v20h-65v17h146V80h-27V60h27V44H237v8m-27%2018v9h54V62l-27-1h-27v9m110%200v9h54V62l-27-1h-27v9M26%2071v8h54V62H26v9m111%200v8h53V62h-53v9m0%2036v9h127V98H137v9M26%20144v8h347v-17H26v9m56%2028l-1%208v8h238v-17H201l-119%201m0%2038l-1%208v7h238v-17H201c-118%200-119%200-119%202m128%205c-3%202-1%204%204%204l5-1h7c0%202%2013%201%2013-1%201-2%200-2-3-2h-3l-1-1-2%201h-8c-5%200-8%201-9%202h-3l2-1%201-1h-3m-92%2040v8h164v-17H118v9\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Protocol Stack\"\n        title=\"Protocol Stack\"\n        src=\"/static/ca442b39d7860130503187fbfe160d89/39600/protocol.png\"\n        srcset=\"/static/ca442b39d7860130503187fbfe160d89/1aaec/protocol.png 175w,\n/static/ca442b39d7860130503187fbfe160d89/98287/protocol.png 350w,\n/static/ca442b39d7860130503187fbfe160d89/39600/protocol.png 700w,\n/static/ca442b39d7860130503187fbfe160d89/83a6d/protocol.png 861w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3>Radio (RF) layer</h3>\n<p>It performs modulation/demodulation of the data into RF signals. It defines the physical characteristics of a Bluetooth transceiver. It defines two types of physical link: connection-less and connection-oriented. </p>\n<h3>Baseband Link layer</h3>\n<p>It performs the connection establishment within a piconet.</p>\n<h3>Link Manager protocol layer</h3>\n<p>It performs the management of the already established links. It also includes authentication and encryption processes.</p>\n<h3>Logical Link Control and Adaptation protocol layer</h3>\n<p>It is also known as the heart of the Bluetooth protocol stack. It allows the communication between upper and lower layers of the Bluetooth protocol stack. It packages the data packets received from upper layers into the form expected by lower layers. It also performs the segmentation and multiplexing.</p>\n<h3>SDP layer</h3>\n<p>It is short for Service Discovery Protocol. It allows discovering the services available on another Bluetooth enabled device.</p>\n<h3>RF comm layer</h3>\n<p>It is short for Radio Frontend Component. It provides a serial interface with WAP and OBEX.</p>\n<h3>OBEX</h3>\n<p>It is short for Object Exchange. It is a communication protocol to exchange objects between two devices.</p>\n<h3>WAP</h3>\n<p>It is short for Wireless Access Protocol. It is used for internet access.</p>\n<h3>TCS</h3>\n<p>It is short for Telephony Control Protocol. It provides telephony service.</p>\n<h3>Application layer</h3>\n<p>It enables the user to interact with the application.</p>\n<h1>Types of Protocol</h1>\n<h2>RFCOMM</h2>\n<p>The RFCOMM protocol provides roughly the same service and reliability guarantees as TCP. Although the specification explicitly states that it was designed to emulate <code class=\"language-text\">RS-232</code> serial ports (to make it easier for manufacturers to add Bluetooth capabilities to their existing serial port devices), it is quite simple to use it in many of the same scenarios as TCP.</p>\n<h3>Implementation</h3>\n<div class=\"gatsby-code-title\">server.py</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> bluetooth <span class=\"token keyword\">import</span> <span class=\"token operator\">*</span>\n \nport <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n \nserver_sock <span class=\"token operator\">=</span> BluetoothSocket<span class=\"token punctuation\">(</span> RFCOMM <span class=\"token punctuation\">)</span>\nserver_sock<span class=\"token punctuation\">.</span>bind<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\"</span><span class=\"token punctuation\">,</span>port<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nserver_sock<span class=\"token punctuation\">.</span>listen<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n \nclient_sock<span class=\"token punctuation\">,</span> client_info <span class=\"token operator\">=</span> server_sock<span class=\"token punctuation\">.</span>accept<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Accepted connection from \"</span><span class=\"token punctuation\">,</span> client_info<span class=\"token punctuation\">)</span>\n \ndata <span class=\"token operator\">=</span> client_sock<span class=\"token punctuation\">.</span>recv<span class=\"token punctuation\">(</span><span class=\"token number\">1024</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"received [%s]\"</span> <span class=\"token operator\">%</span> data<span class=\"token punctuation\">)</span>\n \nclient_sock<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nserver_sock<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-code-title\">client.py</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> bluetooth <span class=\"token keyword\">import</span> <span class=\"token operator\">*</span>\nserver_address <span class=\"token operator\">=</span> <span class=\"token string\">\"01:23:45:67:89:AB\"</span>\nport <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n \nsock<span class=\"token operator\">=</span>BluetoothSocket<span class=\"token punctuation\">(</span> RFCOMM <span class=\"token punctuation\">)</span>\nsock<span class=\"token punctuation\">.</span>connect<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>server_address<span class=\"token punctuation\">,</span> port<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n \nsock<span class=\"token punctuation\">.</span>send<span class=\"token punctuation\">(</span><span class=\"token string\">\"hello\"</span><span class=\"token punctuation\">)</span>\n \nsock<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>L2CAP</h2>\n<p>UDP is often used in situations where reliable delivery of every packet is not crucial, and sometimes to avoid the additional overhead incurred by TCP. Specifically, UDP is chosen for its best-effort, simple datagram semantics. These are the same criteria that L2CAP satisfies as a communications protocol.</p>\n<h3>Implementation</h3>\n<div class=\"gatsby-code-title\">server.py</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> bluetooth <span class=\"token keyword\">import</span> <span class=\"token operator\">*</span>\n \nport <span class=\"token operator\">=</span> <span class=\"token number\">0x1001</span>\n \nserver_sock <span class=\"token operator\">=</span> BluetoothSocket<span class=\"token punctuation\">(</span> L2CAP <span class=\"token punctuation\">)</span>\nserver_sock<span class=\"token punctuation\">.</span>bind<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\"</span><span class=\"token punctuation\">,</span>port<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nserver_sock<span class=\"token punctuation\">.</span>listen<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n \nclient_sock<span class=\"token punctuation\">,</span>address <span class=\"token operator\">=</span> server_sock<span class=\"token punctuation\">.</span>accept<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Accepted connection from \"</span><span class=\"token punctuation\">,</span>address<span class=\"token punctuation\">)</span>\ndata <span class=\"token operator\">=</span> client_sock<span class=\"token punctuation\">.</span>recv<span class=\"token punctuation\">(</span><span class=\"token number\">1024</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"received [%s]\"</span> <span class=\"token operator\">%</span> data<span class=\"token punctuation\">)</span>\n \nclient_sock<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nserver_sock<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> </code></pre></div>\n<div class=\"gatsby-code-title\">client.py</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> bluetooth <span class=\"token keyword\">import</span> <span class=\"token operator\">*</span>\n \nsock <span class=\"token operator\">=</span> BluetoothSocket<span class=\"token punctuation\">(</span>L2CAP<span class=\"token punctuation\">)</span>\n \nbd_addr <span class=\"token operator\">=</span> <span class=\"token string\">\"01:23:45:67:89:AB\"</span>\nport <span class=\"token operator\">=</span> <span class=\"token number\">0x1001</span>\n \nsock<span class=\"token punctuation\">.</span>connect<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>bd_addr<span class=\"token punctuation\">,</span> port<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n \nsock<span class=\"token punctuation\">.</span>send<span class=\"token punctuation\">(</span><span class=\"token string\">\"hello\"</span><span class=\"token punctuation\">)</span>\n \nsock<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>"}},{"node":{"frontmatter":{"title":"OCR Translation","description":"Detecting text based image with OCR for translation","slug":"/blog/ocr-translation","date":"2020-1-1","tags":["python","deeplearning"],"draft":false},"html":"<h1>Abstract</h1>\n<p>Smartphones have been known as the most commonly used electronic devices in daily life today. As the hardware used in the latest smartphones can perform much more intensive tasks than traditional phones, smartphones are no longer just a communication device but also a powerful computing device. It is, for example, possible to apply techniques to perform text detection and translation right from the phone. Therefore, an application that allows smartphones to capture an image and extract the text from it to translate it into other languages is possible now.</p>\n<p>In this study, we have developed a model to extract the text from the image. Final deliverable is tested on many end devices with English and Hindi background and concluded that the application benefits many users. By using this app, travelers who visit India will be able to understand the messages portrayed in Hindi.</p>\n<p>In this project, we will be building a model which can extract the text from image and then translate it into other languages. For this project we are going to use <code class=\"language-text\">googletrans</code> package for the translation and <code class=\"language-text\">tesseract</code> for image recognition.</p>\n<h1>Introduction</h1>\n<p>Current methods only allow the isolated implementations of either OCR or Translation and often, the direct combination of these two independent models only lead to inefficient models that take a lot of time in translating the text in the given image.</p>\n<p>This project implements an efficient model that involves the use of Google’s <code class=\"language-text\">Tesseract</code> module for optical character recognition, <code class=\"language-text\">EAST</code> text detector for segmentation and Googletrans module for translating the recognized text to other languages.</p>\n<p>This allows for achieving the end goal within <code class=\"language-text\">3-4</code> seconds which is an appreciable decrease of processing time which speeds up the entire, thus enabling multiple applications like real-time image translation, video transcripts etc.</p>\n<h1>Model Architecture</h1>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 600px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/67dd6911e5102366141ffb41678ee165/ff59c/architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 100.57142857142858%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'402\\'%20viewBox=\\'0%200%20400%20402\\'%20preserveAspectRatio=\\'none\\'%3e%3cpath%20d=\\'M26%2055c-3%202-3%2017%200%2019%201%202%204%202%2021%202h20v2c0%202%200%203-1%202-2%200-2%200%200%204%202%207%203%207-19%207-17%200-20%200-21%202-2%202-3%2034-1%2036l22%201h20v22c0%2012%200%2022-1%2021-1-3-2%200%200%205%202%206%202%206%204%200%202-5%201-8%200-5-1%201-1-6-1-15v-16h41c39%200%2042%200%2041%202l1%201%206-2c5-1%205-2-1-3l-5-2-1%201c1%201-9%201-40%201H69v-10h20l22-1c2-2%201-34-1-36-1-2-4-2-21-2-22%200-21%200-19-7%202-4%202-4%200-4-1%201-1%200-1-2v-2h20c24%200%2023%201%2023-12%200-8%200-9-2-10-3-2-83-1-84%201m0%209v9l42%201h42V56l-42-1H26v9m148-8v20c2%202%2066%202%2068%200l1-6v-4h22v269h11l1%201h2l4-1c3%200%203%200%202%202l-1%201%2012-3-12-4%201%201c1%202%200%202-8%202h-10V217h10c9%200%2010%200%209%202v1l6-2c5-2%205-2-1-4h-5c1%201-2%201-9%201h-10v-56h10c7%200%209%200%208%201l1%201%205-2c6-1%206-2%201-3a2741%202741%200%2001-7-1c1%202%201%202-2%202l-4-1h-2l-5%201h-5V66h9c8%200%2010%200%209%201%200%202%201%202%205%200%207-2%207-2%203-3l-6-2-2%201c1%201-4%201-20%201h-22v-3l-1-5c-2-2-66-2-68%200m124%201v18c1%202%202%202%2017%202s16%200%2016%202l-1%202v4c2%206%203%207%204%202l2-6-1-1c-2%201-2%201-2-1%200-1%202-2%2017-2h17V56l-34-1c-33%200-34%200-35%202m-123%200c-2%202-1%2016%200%2018l34%201h32V57l-33-1-33%201m125%200c-2%200-1%2018%201%2018%203%202%2063%201%2064%200l1-9-1-9c-1-1-62-2-65%200m-94%2027c-1%204-2%205%200%205%201-1%201%200%201%201%200%202-1%202-19%202l-20%201c-2%201-2%203-2%2037l1%2038c2%202%2080%202%2082%200l1-37c0-33%200-36-2-37-1-2-4-2-20-2-19%200-19%200-19-2%200-1%200-2%201-1v-5c-2-7-3-7-4%200M26%20110v18c1%202%2083%202%2084%200V93H26v17m142-14l-1%208v7h81V95l-40-1c-36%200-40%200-40%202m-1%2026v8h81v-17h-81v9m0%2018l1%209h80v-17h-81v8m131%208l-1%2010c0%2011%200%2011%2019%2011l15%201-1%202v4c2%206%203%207%204%202l2-6h-1c-2%201-2%201-2-1s0-2%2015-2c20%200%2019%200%2019-12%200-8%200-10-2-10a520%20520%200%2000-67%201m1%201v10l1%208h31l34-1%201-9v-8l-33-1-34%201m-131%202v15a694%20694%200%200078%201c2%200%202-2%202-9v-8h-40l-40%201m38%2025c-1%204-2%205%200%205%201-1%201%200%201%201%200%202-1%202-19%202l-21%201c-2%202-2%2073%200%2075l21%201%2019%201-1%206c-2%206-2%206%200%204%201-1%201-1%201%201s0%202-19%202l-20%201c-2%201-2%203-2%2037l1%2038c2%202%2080%202%2082%200l1-37c0-33%200-36-2-37-1-2-4-2-20-2h-19v-2c0-2%200-2%201-1v-6l-2-5h20c19%200%2020%200%2021-2%202-3%201-72-1-73-1-2-4-2-20-2-19%200-19%200-19-2%200-1%200-2%201-1v-5c-2-7-3-7-4%200M26%20186c-2%201-2%202-2%2018l1%2018%2022%201h20v21c0%2018%200%2021-1%2020-2%200-2%200%200%204%202%207%203%207-19%207-25%200-23-2-23%2020l1%2017c1%202%202%202%2022%202h20v15c0%2010%200%2014-1%2013l-1%202%202%206v2H47c-17%200-20%200-21%202-2%201-3%2032-1%2035%201%202%203%202%2043%202%2048%200%2044%201%2044-11v-7h97v-5c0-4%200-5%201-4%201%200%201-4-1-9-1-3-1-3-3%204-2%204-2%205%200%205%201-1%201%200%201%203v4h-95v-8c0-11%201-11-23-11-22%200-21%200-19-6%202-4%202-4%200-4-1%201-1-1-1-8v-9h42a214%20214%200%200142%202l6-2c4-1%204-1-3-3-4-2-5-2-5%200%201%201-6%201-41%201H69v-9h20c20%200%2021%200%2022-2l1-18c0-21%202-19-23-19-22%200-21%200-19-7%202-4%202-4%200-4-1%201-1-1-1-15v-15h42a214%20214%200%200142%202l6-2c4-1%204-1-3-3-4-2-5-2-5%200%201%201-6%201-41%201H69v-9h20l22-1%201-18c0-16%200-17-2-18-2-2-8-2-42-2s-40%200-42%202m0%201v18l1%2017h83v-35l-42-1-42%201m142%201l-1%208v7h81v-16l-40-1c-36%200-40%200-40%202m-1%2026v8h81v-17h-81v9m132-7c-2%201-2%204-2%209%200%2012-1%2012%2018%2012%2015%200%2016%200%2016%202%200%201%200%202-1%201l-1%201%202%205c0%205%202%206%203%201l2-6-1-1c-2%201-2%200-2-1%200-2%201-2%2016-2l17-1c2-2%201-20-1-21l-33-1c-29%200-32%200-33%202m0%201v10l1%208h66v-17l-1-2h-33l-33%201m-132%2024l1%209h80v-17h-81v8m1%2011v15l52%201h28v-17h-40l-40%201M27%20278l-1%2017c0%2014%200%2017%202%2017h81l1-18v-16l-42-1-41%201m141%201l-1%208v7h81v-16l-40-1c-36%200-40%200-40%202m-1%2026v8h81v-17h-81v9m0%2019v8h81v-17h-81v9m132%200c-2%201-2%204-2%209%200%2012-1%2012%2018%2012%2016%200%2016%200%2016%202s0%202-1%201l1%206%201%206%201-4c3-7%203-9%202-8-2%201-2%201-2-1s0-2%2016-2l17-1c2-2%201-20-1-21l-33-1c-29%200-32%200-33%202m0%201v10l1%208h66v-17l-1-2h-33l-33%201m-132%2016c0%2010%200%2010%2013%2010h10v-12h3c4%201%205%206%201%207l-2%203c0%201%204%202%2028%202h28v-17h-81v7M26%20371c0%2015%200%2018%202%2018h80c2%200%202-3%202-18v-17H26v17\\'%20fill=\\'%2364ffda\\'%20fill-rule=\\'evenodd\\'/%3e%3c/svg%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Architecture Diagram\"\n        title=\"Architecture Diagram\"\n        src=\"/static/67dd6911e5102366141ffb41678ee165/ff59c/architecture.png\"\n        srcset=\"/static/67dd6911e5102366141ffb41678ee165/1aaec/architecture.png 175w,\n/static/67dd6911e5102366141ffb41678ee165/98287/architecture.png 350w,\n/static/67dd6911e5102366141ffb41678ee165/ff59c/architecture.png 600w\"\n        sizes=\"(max-width: 600px) 100vw, 600px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h1>Conclusion</h1>\n<p>The model shows an appreciable speed in detecting the text in the images and translating it instantly while maintaining a good average accuracy.</p>"}}]}},"pageContext":{}},"staticQueryHashes":["3115057458"]}